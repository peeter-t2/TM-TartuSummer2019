startsecond	text
15.26	Mark Zuckerberg,
17.26	a journalist was asking him a question about the news feed.
20.26	And the journalist was asking him,
22.26	"""Why is this so important?"""
24.26	And Zuckerberg said,
26.26	"""A squirrel dying in your front yard"
28.26	may be more relevant to your interests right now
31.26	"than people dying in Africa."""
34.26	And I want to talk about
36.26	what a Web based on that idea of relevance might look like.
40.26	So when I was growing up
42.26	in a really rural area in Maine,
44.26	the Internet meant something very different to me.
47.26	It meant a connection to the world.
49.26	It meant something that would connect us all together.
52.26	And I was sure that it was going to be great for democracy
55.26	and for our society.
58.26	But there's this shift
60.26	in how information is flowing online,
62.26	and it's invisible.
65.26	And if we don't pay attention to it,
67.26	it could be a real problem.
70.26	So I first noticed this in a place I spend a lot of time --
73.26	my Facebook page.
75.26	I'm progressive, politically -- big surprise --
78.26	but I've always gone out of my way to meet conservatives.
80.26	I like hearing what they're thinking about;
82.26	I like seeing what they link to;
84.26	I like learning a thing or two.
86.26	And so I was surprised when I noticed one day
89.26	that the conservatives had disappeared from my Facebook feed.
93.26	And what it turned out was going on
95.26	was that Facebook was looking at which links I clicked on,
99.26	and it was noticing that, actually,
101.26	I was clicking more on my liberal friends' links
103.26	than on my conservative friends' links.
106.26	And without consulting me about it,
108.26	it had edited them out.
110.26	They disappeared.
114.26	So Facebook isn't the only place
116.26	that's doing this kind of invisible, algorithmic
118.26	editing of the Web.
121.26	Google's doing it too.
123.26	If I search for something, and you search for something,
126.26	even right now at the very same time,
128.26	we may get very different search results.
131.26	Even if you're logged out, one engineer told me,
134.26	there are 57 signals
136.26	that Google looks at --
139.26	everything from what kind of computer you're on
142.26	to what kind of browser you're using
144.26	to where you're located --
146.26	that it uses to personally tailor your query results.
149.26	Think about it for a second:
151.26	there is no standard Google anymore.
155.26	And you know, the funny thing about this is that it's hard to see.
158.26	You can't see how different your search results are
160.26	from anyone else's.
162.26	But a couple of weeks ago,
164.26	"I asked a bunch of friends to Google ""Egypt"""
167.26	and to send me screen shots of what they got.
170.26	So here's my friend Scott's screen shot.
174.26	And here's my friend Daniel's screen shot.
177.26	When you put them side-by-side,
179.26	you don't even have to read the links
181.26	to see how different these two pages are.
183.26	But when you do read the links,
185.26	it's really quite remarkable.
189.26	Daniel didn't get anything about the protests in Egypt at all
192.26	in his first page of Google results.
194.26	Scott's results were full of them.
196.26	And this was the big story of the day at that time.
198.26	That's how different these results are becoming.
201.26	So it's not just Google and Facebook either.
204.26	This is something that's sweeping the Web.
206.26	There are a whole host of companies that are doing this kind of personalization.
209.26	Yahoo News, the biggest news site on the Internet,
212.26	is now personalized -- different people get different things.
216.26	Huffington Post, the Washington Post, the New York Times --
219.26	all flirting with personalization in various ways.
222.26	And this moves us very quickly
225.26	toward a world in which
227.26	the Internet is showing us what it thinks we want to see,
231.26	but not necessarily what we need to see.
234.26	As Eric Schmidt said,
237.26	"""It will be very hard for people to watch or consume something"
240.26	that has not in some sense
242.26	"been tailored for them."""
245.26	So I do think this is a problem.
247.26	And I think, if you take all of these filters together,
250.26	you take all these algorithms,
252.26	you get what I call a filter bubble.
256.26	And your filter bubble is your own personal,
259.26	unique universe of information
261.26	that you live in online.
263.26	And what's in your filter bubble
266.26	depends on who you are, and it depends on what you do.
269.26	But the thing is that you don't decide what gets in.
273.26	And more importantly,
275.26	you don't actually see what gets edited out.
278.26	So one of the problems with the filter bubble
280.26	was discovered by some researchers at Netflix.
283.26	And they were looking at the Netflix queues, and they noticed something kind of funny
286.26	that a lot of us probably have noticed,
288.26	which is there are some movies
290.26	that just sort of zip right up and out to our houses.
293.26	They enter the queue, they just zip right out.
296.26	"So ""Iron Man"" zips right out,"
298.26	"and ""Waiting for Superman"""
300.26	can wait for a really long time.
302.26	What they discovered
304.26	was that in our Netflix queues
306.26	there's this epic struggle going on
309.26	between our future aspirational selves
312.26	and our more impulsive present selves.
315.26	You know we all want to be someone
317.26	"who has watched ""Rashomon,"""
319.26	but right now
321.26	"we want to watch ""Ace Ventura"" for the fourth time."
324.26	(Laughter)
327.26	So the best editing gives us a bit of both.
329.26	It gives us a little bit of Justin Bieber
331.26	and a little bit of Afghanistan.
333.26	It gives us some information vegetables;
335.26	it gives us some information dessert.
338.26	And the challenge with these kinds of algorithmic filters,
340.26	these personalized filters,
342.26	is that, because they're mainly looking
344.26	at what you click on first,
348.26	it can throw off that balance.
352.26	And instead of a balanced information diet,
355.26	you can end up surrounded
357.26	by information junk food.
359.26	What this suggests
361.26	is actually that we may have the story about the Internet wrong.
364.26	In a broadcast society --
366.26	this is how the founding mythology goes --
368.26	in a broadcast society,
370.26	there were these gatekeepers, the editors,
372.26	and they controlled the flows of information.
375.26	And along came the Internet and it swept them out of the way,
378.26	and it allowed all of us to connect together,
380.26	and it was awesome.
382.26	But that's not actually what's happening right now.
386.26	What we're seeing is more of a passing of the torch
389.26	from human gatekeepers
391.26	to algorithmic ones.
394.26	And the thing is that the algorithms
397.26	don't yet have the kind of embedded ethics
400.26	that the editors did.
403.26	So if algorithms are going to curate the world for us,
406.26	if they're going to decide what we get to see and what we don't get to see,
409.26	then we need to make sure
411.26	that they're not just keyed to relevance.
414.26	We need to make sure that they also show us things
416.26	that are uncomfortable or challenging or important --
419.26	this is what TED does --
421.26	other points of view.
423.26	And the thing is, we've actually been here before
425.26	as a society.
428.26	In 1915, it's not like newspapers were sweating a lot
431.26	about their civic responsibilities.
434.26	Then people noticed
436.26	that they were doing something really important.
439.26	That, in fact, you couldn't have
441.26	a functioning democracy
443.26	if citizens didn't get a good flow of information,
448.26	that the newspapers were critical because they were acting as the filter,
451.26	and then journalistic ethics developed.
453.26	It wasn't perfect,
455.26	but it got us through the last century.
458.26	And so now,
460.26	we're kind of back in 1915 on the Web.
464.26	And we need the new gatekeepers
467.26	to encode that kind of responsibility
469.26	into the code that they're writing.
471.26	I know that there are a lot of people here from Facebook and from Google --
474.26	Larry and Sergey --
476.26	people who have helped build the Web as it is,
478.26	and I'm grateful for that.
480.26	But we really need you to make sure
483.26	that these algorithms have encoded in them
486.26	a sense of the public life, a sense of civic responsibility.
489.26	We need you to make sure that they're transparent enough
492.26	that we can see what the rules are
494.26	that determine what gets through our filters.
497.26	And we need you to give us some control
499.26	so that we can decide
501.26	what gets through and what doesn't.
504.26	Because I think
506.26	we really need the Internet to be that thing
508.26	that we all dreamed of it being.
510.26	We need it to connect us all together.
513.26	We need it to introduce us to new ideas
516.26	and new people and different perspectives.
520.26	And it's not going to do that
522.26	if it leaves us all isolated in a Web of one.
525.26	Thank you.
527.26	(Applause)
