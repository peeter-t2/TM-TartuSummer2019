startsecond	text
13	I'm going to talk
13	about a failure of intuition
15.24	that many of us suffer from.
17.48	It's really a failure
17.48	to detect a certain kind of danger.
21.36	I'm going to describe a scenario
23.12	that I think is both terrifying
26.4	and likely to occur,
28.84	and that's not a good combination,
30.52	as it turns out.
32.08	And yet rather than be scared,
32.08	most of you will feel
34.56	that what I'm talking about
34.56	is kind of cool.
37.2	I'm going to describe
37.2	how the gains we make
40.2	in artificial intelligence
42	could ultimately destroy us.
43.8	And in fact, I think it's very difficult
43.8	to see how they won't destroy us
47.28	or inspire us to destroy ourselves.
49.4	And yet if you're anything like me,
51.28	you'll find that it's fun
51.28	to think about these things.
53.96	And that response is part of the problem.
57.36	OK? That response should worry you.
59.92	And if I were to convince you in this talk
62.6	that we were likely
62.6	to suffer a global famine,
66.04	either because of climate change
66.04	or some other catastrophe,
69.12	and that your grandchildren,
69.12	or their grandchildren,
72.56	are very likely to live like this,
75.2	you wouldn't think,
77.44	"""Interesting."
78.8	"I like this TED Talk."""
81.2	Famine isn't fun.
83.8	Death by science fiction,
83.8	on the other hand, is fun,
87.2	and one of the things that worries me most
87.2	about the development of AI at this point
91.2	is that we seem unable to marshal
91.2	an appropriate emotional response
95.32	to the dangers that lie ahead.
97.16	I am unable to marshal this response,
97.16	and I'm giving this talk.
102.12	It's as though we stand before two doors.
104.84	Behind door number one,
106.12	we stop making progress
106.12	in building intelligent machines.
109.44	Our computer hardware and software
109.44	just stops getting better for some reason.
113.48	Now take a moment
113.48	to consider why this might happen.
117.08	I mean, given how valuable
117.08	intelligence and automation are,
120.76	we will continue to improve our technology
120.76	if we are at all able to.
125.2	What could stop us from doing this?
127.8	A full-scale nuclear war?
131	A global pandemic?
134.32	An asteroid impact?
137.64	Justin Bieber becoming
137.64	president of the United States?
140.24	(Laughter)
144.76	The point is, something would have to
144.76	destroy civilization as we know it.
149.36	You have to imagine
149.36	how bad it would have to be
153.68	to prevent us from making
153.68	improvements in our technology
157.04	permanently,
158.28	generation after generation.
160.32	Almost by definition,
160.32	this is the worst thing
162.48	that's ever happened in human history.
164.52	So the only alternative,
165.84	and this is what lies
165.84	behind door number two,
168.2	is that we continue
168.2	to improve our intelligent machines
171.36	year after year after year.
173.72	At a certain point, we will build
173.72	machines that are smarter than we are,
178.08	and once we have machines
178.08	that are smarter than we are,
180.72	they will begin to improve themselves.
182.72	And then we risk what
182.72	the mathematician IJ Good called
185.48	"an ""intelligence explosion,"""
187.28	that the process could get away from us.
190.12	Now, this is often caricatured,
190.12	as I have here,
192.96	as a fear that armies of malicious robots
196.2	will attack us.
197.48	But that isn't the most likely scenario.
200.2	It's not that our machines
200.2	will become spontaneously malevolent.
205.08	The concern is really
205.08	that we will build machines
207.72	that are so much
207.72	more competent than we are
209.8	that the slightest divergence
209.8	between their goals and our own
213.6	could destroy us.
215.96	Just think about how we relate to ants.
218.6	We don't hate them.
220.28	We don't go out of our way to harm them.
222.36	In fact, sometimes
222.36	we take pains not to harm them.
224.76	We step over them on the sidewalk.
226.8	But whenever their presence
228.96	seriously conflicts with one of our goals,
231.48	let's say when constructing
231.48	a building like this one,
233.981	we annihilate them without a qualm.
236.48	The concern is that we will
236.48	one day build machines
239.44	that, whether they're conscious or not,
242.2	could treat us with similar disregard.
245.76	Now, I suspect this seems
245.76	far-fetched to many of you.
249.36	I bet there are those of you who doubt
249.36	that superintelligent AI is possible,
255.72	much less inevitable.
257.4	But then you must find something wrong
257.4	with one of the following assumptions.
261.044	And there are only three of them.
263.8	Intelligence is a matter of information
263.8	processing in physical systems.
269.32	Actually, this is a little bit more
269.32	than an assumption.
271.959	We have already built
271.959	narrow intelligence into our machines,
275.44	and many of these machines perform
277.48	at a level of superhuman
277.48	intelligence already.
280.84	And we know that mere matter
283.44	can give rise to what is called
283.44	"""general intelligence,"""
286.08	an ability to think flexibly
286.08	across multiple domains,
289.76	because our brains have managed it. Right?
292.92	I mean, there's just atoms in here,
296.88	and as long as we continue
296.88	to build systems of atoms
301.4	that display more and more
301.4	intelligent behavior,
304.12	we will eventually,
304.12	unless we are interrupted,
306.68	we will eventually
306.68	build general intelligence
310.08	into our machines.
311.4	It's crucial to realize
311.4	that the rate of progress doesn't matter,
315.08	because any progress
315.08	is enough to get us into the end zone.
318.28	We don't need Moore's law to continue.
318.28	We don't need exponential progress.
322.08	We just need to keep going.
325.48	The second assumption
325.48	is that we will keep going.
329	We will continue to improve
329	our intelligent machines.
333	And given the value of intelligence --
337.4	I mean, intelligence is either
337.4	the source of everything we value
340.96	or we need it to safeguard
340.96	everything we value.
343.76	It is our most valuable resource.
346.04	So we want to do this.
347.6	We have problems
347.6	that we desperately need to solve.
350.96	We want to cure diseases
350.96	like Alzheimer's and cancer.
354.96	We want to understand economic systems.
354.96	We want to improve our climate science.
358.92	So we will do this, if we can.
361.2	The train is already out of the station,
361.2	and there's no brake to pull.
365.88	Finally, we don't stand
365.88	on a peak of intelligence,
371.36	or anywhere near it, likely.
373.64	And this really is the crucial insight.
375.56	This is what makes
375.56	our situation so precarious,
378	and this is what makes our intuitions
378	about risk so unreliable.
383.12	Now, just consider the smartest person
383.12	who has ever lived.
386.64	On almost everyone's shortlist here
386.64	is John von Neumann.
390.08	I mean, the impression that von Neumann
390.08	made on the people around him,
393.44	and this included the greatest
393.44	mathematicians and physicists of his time,
397.52	is fairly well-documented.
399.48	If only half the stories
399.48	about him are half true,
403.28	there's no question
404.52	he's one of the smartest people
404.52	who has ever lived.
407	So consider the spectrum of intelligence.
410.32	Here we have John von Neumann.
413.56	And then we have you and me.
416.12	And then we have a chicken.
417.44	(Laughter)
419.4	Sorry, a chicken.
420.64	(Laughter)
421.92	There's no reason for me to make this talk
421.92	more depressing than it needs to be.
425.68	(Laughter)
428.339	It seems overwhelmingly likely, however,
428.339	that the spectrum of intelligence
431.84	extends much further
431.84	than we currently conceive,
435.88	and if we build machines
435.88	that are more intelligent than we are,
439.12	they will very likely
439.12	explore this spectrum
441.44	in ways that we can't imagine,
443.32	and exceed us in ways
443.32	that we can't imagine.
447	And it's important to recognize that
447	this is true by virtue of speed alone.
451.36	Right? So imagine if we just built
451.36	a superintelligent AI
456.44	that was no smarter
456.44	than your average team of researchers
459.92	at Stanford or MIT.
462.24	Well, electronic circuits
462.24	function about a million times faster
465.24	than biochemical ones,
466.52	so this machine should think
466.52	about a million times faster
469.68	than the minds that built it.
471.52	So you set it running for a week,
473.2	and it will perform 20,000 years
473.2	of human-level intellectual work,
478.4	week after week after week.
481.64	How could we even understand,
481.64	much less constrain,
484.76	a mind making this sort of progress?
488.84	The other thing that's worrying, frankly,
491	is that, imagine the best case scenario.
496	So imagine we hit upon a design
496	of superintelligent AI
500.2	that has no safety concerns.
501.6	We have the perfect design
501.6	the first time around.
504.88	It's as though we've been handed an oracle
507.12	that behaves exactly as intended.
509.16	Well, this machine would be
509.16	the perfect labor-saving device.
513.68	It can design the machine
513.68	that can build the machine
516.133	that can do any physical work,
517.92	powered by sunlight,
519.4	more or less for the cost
519.4	of raw materials.
522.12	So we're talking about
522.12	the end of human drudgery.
525.4	We're also talking about the end
525.4	of most intellectual work.
529.2	So what would apes like ourselves
529.2	do in this circumstance?
532.28	Well, we'd be free to play Frisbee
532.28	and give each other massages.
537.84	Add some LSD and some
537.84	questionable wardrobe choices,
540.72	and the whole world
540.72	could be like Burning Man.
542.92	(Laughter)
546.32	Now, that might sound pretty good,
549.28	but ask yourself what would happen
551.68	under our current economic
551.68	and political order?
554.44	It seems likely that we would witness
556.88	a level of wealth inequality
556.88	and unemployment
561.04	that we have never seen before.
562.56	Absent a willingness
562.56	to immediately put this new wealth
565.2	to the service of all humanity,
567.64	a few trillionaires could grace
567.64	the covers of our business magazines
571.28	while the rest of the world
571.28	would be free to starve.
574.32	And what would the Russians
574.32	or the Chinese do
576.64	if they heard that some company
576.64	in Silicon Valley
579.28	was about to deploy a superintelligent AI?
582.04	This machine would be capable
582.04	of waging war,
584.92	whether terrestrial or cyber,
587.16	with unprecedented power.
590.12	This is a winner-take-all scenario.
592	To be six months ahead
592	of the competition here
595.16	is to be 500,000 years ahead,
597.96	at a minimum.
599.48	So it seems that even mere rumors
599.48	of this kind of breakthrough
604.24	could cause our species to go berserk.
606.64	Now, one of the most frightening things,
609.56	in my view, at this moment,
612.36	are the kinds of things
612.36	that AI researchers say
616.68	when they want to be reassuring.
619	And the most common reason
619	we're told not to worry is time.
622.48	This is all a long way off,
622.48	don't you know.
624.56	This is probably 50 or 100 years away.
627.72	One researcher has said,
629	"""Worrying about AI safety"
630.6	is like worrying
630.6	"about overpopulation on Mars."""
634.116	This is the Silicon Valley version
635.76	"of ""don't worry your"
635.76	"pretty little head about it."""
638.16	(Laughter)
639.52	No one seems to notice
641.44	that referencing the time horizon
644.08	is a total non sequitur.
646.68	If intelligence is just a matter
646.68	of information processing,
649.96	and we continue to improve our machines,
652.64	we will produce
652.64	some form of superintelligence.
656.32	And we have no idea
656.32	how long it will take us
660	to create the conditions
660	to do that safely.
664.2	Let me say that again.
665.52	We have no idea how long it will take us
669.36	to create the conditions
669.36	to do that safely.
672.92	And if you haven't noticed,
676.4	This is 50 years in months.
678.88	This is how long we've had the iPhone.
681.44	"This is how long ""The Simpsons"""
681.44	has been on television.
684.68	Fifty years is not that much time
687.08	to meet one of the greatest challenges
687.08	our species will ever face.
691.64	Once again, we seem to be failing
691.64	to have an appropriate emotional response
695.68	to what we have every reason
695.68	to believe is coming.
698.4	The computer scientist Stuart Russell
698.4	has a nice analogy here.
702.4	He said, imagine that we received
702.4	a message from an alien civilization,
707.32	which read:
709.04	"""People of Earth,"
710.6	we will arrive on your planet in 50 years.
713.8	"Get ready."""
715.4	And now we're just counting down
715.4	the months until the mothership lands?
719.68	We would feel a little
719.68	more urgency than we do.
724.68	Another reason we're told not to worry
726.56	is that these machines
726.56	can't help but share our values
729.6	because they will be literally
729.6	extensions of ourselves.
732.24	They'll be grafted onto our brains,
734.08	and we'll essentially
734.08	become their limbic systems.
737.12	Now take a moment to consider
738.56	that the safest
738.56	and only prudent path forward,
741.76	recommended,
743.12	is to implant this technology
743.12	directly into our brains.
746.6	Now, this may in fact be the safest
746.6	and only prudent path forward,
750	but usually one's safety concerns
750	about a technology
753.08	have to be pretty much worked out
753.08	before you stick it inside your head.
756.76	(Laughter)
758.8	The deeper problem is that
758.8	building superintelligent AI on its own
764.16	seems likely to be easier
765.92	than building superintelligent AI
767.8	and having the completed neuroscience
769.6	that allows us to seamlessly
769.6	integrate our minds with it.
772.8	And given that the companies
772.8	and governments doing this work
776	are likely to perceive themselves
776	as being in a race against all others,
779.68	given that to win this race
779.68	is to win the world,
782.96	provided you don't destroy it
782.96	in the next moment,
785.44	then it seems likely
785.44	that whatever is easier to do
788.08	will get done first.
790.56	Now, unfortunately,
790.56	I don't have a solution to this problem,
793.44	apart from recommending
793.44	that more of us think about it.
796.08	I think we need something
796.08	like a Manhattan Project
798.48	on the topic of artificial intelligence.
800.52	Not to build it, because I think
800.52	we'll inevitably do that,
803.28	but to understand
803.28	how to avoid an arms race
806.64	and to build it in a way
806.64	that is aligned with our interests.
810.16	When you're talking
810.16	about superintelligent AI
812.32	that can make changes to itself,
814.6	it seems that we only have one chance
814.6	to get the initial conditions right,
819.24	and even then we will need to absorb
821.32	the economic and political
821.32	consequences of getting them right.
825.76	But the moment we admit
827.84	that information processing
827.84	is the source of intelligence,
832.72	that some appropriate computational system
832.72	is what the basis of intelligence is,
838.36	and we admit that we will improve
838.36	these systems continuously,
843.28	and we admit that the horizon
843.28	of cognition very likely far exceeds
847.76	what we currently know,
850.12	then we have to admit
851.36	that we are in the process
851.36	of building some sort of god.
855.4	Now would be a good time
857	to make sure it's a god we can live with.
860.12	Thank you very much.
861.68	(Applause)
