startsecond	text
18.33	Cultural evolution is a dangerous child
21.33	for any species to let loose on its planet.
24.33	By the time you realize what's happening, the child is a toddler,
28.33	up and causing havoc, and it's too late to put it back.
34.33	We humans are Earth's Pandoran species.
37.33	We're the ones who let the second replicator out of its box,
42.33	and we can't push it back in.
44.33	We're seeing the consequences all around us.
48.33	Now that, I suggest, is the view that
52.33	comes out of taking memetics seriously.
54.33	And it gives us a new way of thinking about
56.33	not only what's going on on our planet,
58.33	but what might be going on elsewhere in the cosmos.
61.33	So first of all, I'd like to say something about memetics
64.33	and the theory of memes,
66.33	and secondly, how this might answer questions about who's out there,
71.33	if indeed anyone is.
74.33	So, memetics:
76.33	memetics is founded on the principle of Universal Darwinism.
80.33	Darwin had this amazing idea.
83.33	Indeed, some people say
85.33	it's the best idea anybody ever had.
88.33	Isn't that a wonderful thought, that there could be such a thing
92.33	as a best idea anybody ever had?
94.33	Do you think there could?
95.33	Audience: No.
96.33	(Laughter)
97.33	Susan Blackmore: Someone says no, very loudly, from over there.
99.33	Well, I say yes, and if there is, I give the prize to Darwin.
103.33	Why?
105.33	Because the idea was so simple,
108.33	and yet it explains all design in the universe.
114.33	I would say not just biological design,
116.33	but all of the design that we think of as human design.
118.33	It's all just the same thing happening.
120.33	What did Darwin say?
122.33	I know you know the idea, natural selection,
124.33	"but let me just paraphrase ""The Origin of Species,"" 1859,"
129.33	in a few sentences.
131.33	What Darwin said was something like this:
134.33	if you have creatures that vary, and that can't be doubted --
138.33	I've been to the Galapagos, and I've measured the size of the beaks
141.33	and the size of the turtle shells and so on, and so on.
143.33	And 100 pages later.
145.33	(Laughter)
147.33	And if there is a struggle for life,
151.33	such that nearly all of these creatures die --
154.33	and this can't be doubted, I've read Malthus
157.33	and I've calculated how long it would take for elephants
159.33	to cover the whole world if they bred unrestricted, and so on and so on.
162.33	And another 100 pages later.
166.33	And if the very few that survive pass onto their offspring
171.33	whatever it was that helped them survive,
174.33	then those offspring must be better adapted
176.33	to the circumstances in which all this happened
178.33	than their parents were.
181.33	You see the idea?
183.33	If, if, if, then.
185.33	He had no concept of the idea of an algorithm,
187.33	but that's what he described in that book,
190.33	and this is what we now know as the evolutionary algorithm.
193.33	The principle is you just need those three things --
197.33	variation, selection and heredity.
200.33	And as Dan Dennett puts it, if you have those,
204.33	then you must get evolution.
206.33	Or design out of chaos, without the aid of mind.
211.33	There's one word I love on that slide.
213.33	What do you think my favorite word is?
215.33	Audience: Chaos.
216.33	SB: Chaos? No. What? Mind? No.
219.33	Audience: Without.
220.33	SB: No, not without.
221.33	(Laughter)
222.33	You try them all in order: Mmm...?
224.33	Audience: Must.
225.33	SB: Must, at must. Must, must.
229.33	This is what makes it so amazing.
231.33	You don't need a designer,
234.33	or a plan, or foresight, or anything else.
237.33	If there's something that is copied with variation
240.33	and it's selected, then you must get design appearing out of nowhere.
244.33	You can't stop it.
246.33	Must is my favorite word there.
251.33	Now, what's this to do with memes?
253.33	Well, the principle here applies to anything
258.33	that is copied with variation and selection.
259.33	We're so used to thinking in terms of biology,
262.33	we think about genes this way.
264.33	Darwin didn't, of course; he didn't know about genes.
267.33	He talked mostly about animals and plants,
269.33	but also about languages evolving and becoming extinct.
272.33	But the principle of Universal Darwinism
274.33	is that any information that is varied and selected
278.33	will produce design.
280.33	And this is what Richard Dawkins was on about
282.33	"in his 1976 bestseller, ""The Selfish Gene."""
285.33	The information that is copied, he called the replicator.
289.33	It selfishly copies.
291.33	"Not meaning it kind of sits around inside cells going, ""I want to get copied."""
295.33	But that it will get copied if it can,
297.33	regardless of the consequences.
300.33	It doesn't care about the consequences because it can't,
303.33	because it's just information being copied.
306.33	And he wanted to get away
307.33	from everybody thinking all the time about genes,
310.33	"and so he said, ""Is there another replicator out there on the planet?"""
313.33	Ah, yes, there is.
315.33	Look around you -- here will do, in this room.
318.33	All around us, still clumsily drifting about
321.33	in its primeval soup of culture, is another replicator.
324.33	Information that we copy from person to person, by imitation,
329.33	by language, by talking, by telling stories,
331.33	by wearing clothes, by doing things.
334.33	This is information copied with variation and selection.
339.33	This is design process going on.
342.33	He wanted a name for the new replicator.
345.33	"So, he took the Greek word ""mimeme,"" which means that which is imitated."
349.33	Remember that, that's the core definition:
352.33	that which is imitated.
353.33	And abbreviated it to meme, just because it sounds good
356.33	and made a good meme, an effective spreading meme.
359.33	So that's how the idea came about.
363.33	It's important to stick with that definition.
366.33	The whole science of memetics is much maligned,
370.33	much misunderstood, much feared.
373.33	But a lot of these problems can be avoided
376.33	by remembering the definition.
378.33	A meme is not equivalent to an idea.
380.33	It's not an idea. It's not equivalent to anything else, really.
382.33	Stick with the definition.
384.33	It's that which is imitated,
386.33	or information which is copied from person to person.
390.33	So, let's see some memes.
391.33	Well, you sir, you've got those glasses hung around your neck
394.33	in that particularly fetching way.
396.33	I wonder whether you invented that idea for yourself,
398.33	or copied it from someone else?
400.33	If you copied it from someone else, it's a meme.
403.33	And what about, oh, I can't see any interesting memes here.
406.33	All right everyone, who's got some interesting memes for me?
409.33	Oh, well, your earrings,
411.33	I don't suppose you invented the idea of earrings.
413.33	You probably went out and bought them.
415.33	There are plenty more in the shops.
417.33	That's something that's passed on from person to person.
419.33	All the stories that we're telling -- well, of course,
422.33	TED is a great meme-fest, masses of memes.
426.33	The way to think about memes, though,
428.33	is to think, why do they spread?
430.33	They're selfish information, they will get copied, if they can.
434.33	But some of them will be copied because they're good,
437.33	or true, or useful, or beautiful.
439.33	Some of them will be copied even though they're not.
441.33	Some, it's quite hard to tell why.
444.33	There's one particular curious meme which I rather enjoy.
447.33	And I'm glad to say, as I expected, I found it when I came here,
450.33	and I'm sure all of you found it, too.
452.33	You go to your nice, posh, international hotel somewhere,
456.33	and you come in and you put down your clothes
458.33	and you go to the bathroom, and what do you see?
461.33	Audience: Bathroom soap.
462.33	SB: Pardon?
463.33	Audience: Soap.
464.33	SB: Soap, yeah. What else do you see?
466.33	Audience: (Inaudible)
467.33	SB: Mmm mmm.
468.33	Audience: Sink, toilet!
469.33	SB: Sink, toilet, yes, these are all memes, they're all memes,
471.33	but they're sort of useful ones, and then there's this one.
474.33	(Laughter)
478.33	What is this one doing?
480.33	(Laughter)
481.33	This has spread all over the world.
483.33	It's not surprising that you all found it
485.33	when you arrived in your bathrooms here.
487.33	But I took this photograph in a toilet at the back of a tent
492.33	in the eco-camp in the jungle in Assam.
494.33	(Laughter)
496.33	Who folded that thing up there, and why?
499.33	(Laughter)
500.33	Some people get carried away.
502.33	(Laughter)
506.33	Other people are just lazy and make mistakes.
509.33	Some hotels exploit the opportunity to put even more memes
512.33	with a little sticker.
514.33	(Laughter)
515.33	What is this all about?
517.33	I suppose it's there to tell you that somebody's
519.33	cleaned the place, and it's all lovely.
521.33	And you know, actually, all it tells you is that another person
524.33	has potentially spread germs from place to place.
527.33	(Laughter)
528.33	So, think of it this way.
530.33	Imagine a world full of brains
532.33	and far more memes than can possibly find homes.
535.33	The memes are all trying to get copied --
538.33	trying, in inverted commas -- i.e.,
541.33	that's the shorthand for, if they can get copied, they will.
544.33	They're using you and me as their propagating, copying machinery,
550.33	and we are the meme machines.
553.33	Now, why is this important?
555.33	Why is this useful, or what does it tell us?
557.33	It gives us a completely new view of human origins
561.33	and what it means to be human,
562.33	all conventional theories of cultural evolution,
566.33	of the origin of humans,
568.33	and what makes us so different from other species.
572.33	All other theories explaining the big brain, and language, and tool use
574.33	and all these things that make us unique,
576.33	are based upon genes.
579.33	Language must have been useful for the genes.
582.33	Tool use must have enhanced our survival, mating and so on.
585.33	It always comes back, as Richard Dawkins complained
588.33	all that long time ago, it always comes back to genes.
591.33	"The point of memetics is to say, ""Oh no, it doesn't."""
595.33	There are two replicators now on this planet.
598.33	From the moment that our ancestors,
601.33	perhaps two and a half million years ago or so,
603.33	began imitating, there was a new copying process.
607.33	Copying with variation and selection.
609.33	A new replicator was let loose, and it could never be --
614.33	right from the start -- it could never be
615.33	that human beings who let loose this new creature,
620.33	could just copy the useful, beautiful, true things,
623.33	and not copy the other things.
625.33	While their brains were having an advantage from being able to copy --
628.33	lighting fires, keeping fires going, new techniques of hunting,
633.33	these kinds of things --
635.33	inevitably they were also copying putting feathers in their hair,
638.33	or wearing strange clothes, or painting their faces,
640.33	or whatever.
641.33	So, you get an arms race between the genes
645.33	which are trying to get the humans to have small economical brains
649.33	and not waste their time copying all this stuff,
651.33	and the memes themselves, like the sounds that people made and copied --
656.33	in other words, what turned out to be language --
658.33	competing to get the brains to get bigger and bigger.
661.33	So, the big brain, on this theory, is driven by the memes.
665.33	"This is why, in ""The Meme Machine,"" I called it memetic drive."
669.33	As the memes evolve, as they inevitably must,
672.33	they drive a bigger brain that is better at copying the memes
676.33	that are doing the driving.
678.33	This is why we've ended up with such peculiar brains,
682.33	that we like religion, and music, and art.
685.33	Language is a parasite that we've adapted to,
688.33	not something that was there originally for our genes,
690.33	on this view.
692.33	And like most parasites, it can begin dangerous,
695.33	but then it coevolves and adapts,
698.33	and we end up with a symbiotic relationship
700.33	with this new parasite.
701.33	And so, from our perspective,
703.33	we don't realize that that's how it began.
706.33	So, this is a view of what humans are.
709.33	All other species on this planet are gene machines only,
712.33	they don't imitate at all well, hardly at all.
715.33	We alone are gene machines and meme machines as well.
720.33	The memes took a gene machine and turned it into a meme machine.
724.33	But that's not all.
726.33	We have a new kind of memes now.
729.33	I've been wondering for a long time,
730.33	since I've been thinking about memes a lot,
732.33	is there a difference between the memes that we copy --
734.33	the words we speak to each other,
736.33	the gestures we copy, the human things --
738.33	and all these technological things around us?
740.33	I have always, until now, called them all memes,
744.33	but I do honestly think now
747.33	we need a new word for technological memes.
750.33	Let's call them techno-memes or temes.
753.33	Because the processes are getting different.
757.33	We began, perhaps 5,000 years ago, with writing.
760.33	We put the storage of memes out there on a clay tablet,
768.33	but in order to get true temes and true teme machines,
770.33	you need to get the variation, the selection and the copying,
773.33	all done outside of humans.
775.33	And we're getting there.
777.33	We're at this extraordinary point where we're nearly there,
779.33	that there are machines like that.
781.33	And indeed, in the short time I've already been at TED,
783.33	I see we're even closer than I thought we were before.
785.33	So actually, now the temes are forcing our brains
791.33	to become more like teme machines.
793.33	Our children are growing up very quickly learning to read,
796.33	learning to use machinery.
798.33	We're going to have all kinds of implants,
799.33	drugs that force us to stay awake all the time.
802.33	We'll think we're choosing these things,
804.33	but the temes are making us do it.
808.33	So, we're at this cusp now
809.33	of having a third replicator on our planet.
814.33	Now, what about what else is going on out there in the universe?
819.33	Is there anyone else out there?
821.33	People have been asking this question for a long time.
824.33	We've been asking it here at TED already.
826.33	In 1961, Frank Drake made his famous equation,
830.33	but I think he concentrated on the wrong things.
832.33	It's been very productive, that equation.
834.33	He wanted to estimate N,
836.33	the number of communicative civilizations out there in our galaxy,
840.33	and he included in there the rate of star formation,
844.33	the rate of planets, but crucially, intelligence.
848.33	I think that's the wrong way to think about it.
852.33	Intelligence appears all over the place, in all kinds of guises.
855.33	Human intelligence is only one kind of a thing.
857.33	But what's really important is the replicators you have
860.33	and the levels of replicators, one feeding on the one before.
864.33	So, I would suggest that we don't think intelligence,
869.33	we think replicators.
871.33	And on that basis, I've suggested a different kind of equation.
874.33	A very simple equation.
876.33	N, the same thing,
878.33	the number of communicative civilizations out there
881.33	[that] we might expect in our galaxy.
883.33	Just start with the number of planets there are in our galaxy.
887.33	The fraction of those which get a first replicator.
891.33	The fraction of those that get the second replicator.
895.33	The fraction of those that get the third replicator.
898.33	Because it's only the third replicator that's going to reach out --
901.33	sending information, sending probes, getting out there,
904.33	and communicating with anywhere else.
906.33	OK, so if we take that equation,
909.33	why haven't we heard from anybody out there?
914.33	Because every step is dangerous.
918.33	Getting a new replicator is dangerous.
921.33	You can pull through, we have pulled through,
923.33	but it's dangerous.
925.33	Take the first step, as soon as life appeared on this earth.
928.33	We may take the Gaian view.
930.33	I loved Peter Ward's talk yesterday -- it's not Gaian all the time.
933.33	Actually, life forms produce things that kill themselves.
936.33	Well, we did pull through on this planet.
939.33	But then, a long time later, billions of years later,
941.33	we got the second replicator, the memes.
944.33	That was dangerous, all right.
946.33	Think of the big brain.
948.33	How many mothers do we have here?
951.33	You know all about big brains.
953.33	They are dangerous to give birth to,
955.33	are agonizing to give birth to.
957.33	(Laughter)
959.33	My cat gave birth to four kittens, purring all the time.
961.33	Ah, mm -- slightly different.
963.33	(Laughter)
965.33	But not only is it painful, it kills lots of babies,
968.33	it kills lots of mothers,
970.33	and it's very expensive to produce.
972.33	The genes are forced into producing all this myelin,
974.33	all the fat to myelinate the brain.
976.33	Do you know, sitting here,
978.33	your brain is using about 20 percent of your body's energy output
982.33	for two percent of your body weight?
984.33	It's a really expensive organ to run.
986.33	Why? Because it's producing the memes.
988.33	Now, it could have killed us off. It could have killed us off,
992.33	and maybe it nearly did, but you see, we don't know.
994.33	But maybe it nearly did.
996.33	Has it been tried before?
997.33	What about all those other species?
999.33	Louise Leakey talked yesterday
1001.33	about how we're the only one in this branch left.
1004.33	What happened to the others?
1006.33	Could it be that this experiment in imitation,
1008.33	this experiment in a second replicator,
1010.33	is dangerous enough to kill people off?
1014.33	Well, we did pull through, and we adapted.
1016.33	But now, we're hitting, as I've just described,
1019.33	we're hitting the third replicator point.
1021.33	And this is even more dangerous --
1024.33	well, it's dangerous again.
1026.33	Why? Because the temes are selfish replicators
1030.33	and they don't care about us, or our planet, or anything else.
1033.33	They're just information, why would they?
1037.33	They are using us to suck up the planet's resources
1039.33	to produce more computers,
1041.33	and more of all these amazing things we're hearing about here at TED.
1044.33	"Don't think, ""Oh, we created the Internet for our own benefit."""
1048.33	That's how it seems to us.
1050.33	Think, temes spreading because they must.
1054.33	We are the old machines.
1056.33	Now, are we going to pull through?
1058.33	What's going to happen?
1060.33	What does it mean to pull through?
1062.33	Well, there are kind of two ways of pulling through.
1065.33	One that is obviously happening all around us now,
1067.33	is that the temes turn us into teme machines,
1071.33	with these implants, with the drugs,
1073.33	with us merging with the technology.
1076.33	And why would they do that?
1078.33	Because we are self-replicating.
1080.33	We have babies.
1082.33	We make new ones, and so it's convenient to piggyback on us,
1085.33	because we're not yet at the stage on this planet
1089.33	where the other option is viable.
1091.33	Although it's closer, I heard this morning,
1093.33	it's closer than I thought it was.
1095.33	Where the teme machines themselves will replicate themselves.
1098.33	That way, it wouldn't matter if the planet's climate
1102.33	was utterly destabilized,
1104.33	and it was no longer possible for humans to live here.
1106.33	Because those teme machines, they wouldn't need --
1108.33	they're not squishy, wet, oxygen-breathing,
1110.33	warmth-requiring creatures.
1113.33	They could carry on without us.
1115.33	So, those are the two possibilities.
1118.33	The second, I don't think we're that close.
1122.33	It's coming, but we're not there yet.
1124.33	The first, it's coming too.
1126.33	But the damage that is already being done
1129.33	to the planet is showing us how dangerous the third point is,
1134.33	that third danger point, getting a third replicator.
1138.33	And will we get through this third danger point,
1140.33	like we got through the second and like we got through the first?
1144.33	Maybe we will, maybe we won't.
1146.33	I have no idea.
1153.33	(Applause)
1164.33	Chris Anderson: That was an incredible talk.
1166.33	SB: Thank you. I scared myself.
1168.33	CA: (Laughter)
