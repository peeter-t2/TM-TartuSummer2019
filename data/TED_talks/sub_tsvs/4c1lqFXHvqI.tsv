startsecond	text
12.973	We are built out of very small stuff,
17.516	and we are embedded in
17.516	a very large cosmos,
20.031	and the fact is that we are not
20.031	very good at understanding reality
24.582	at either of those scales,
26.161	and that's because our brains
27.763	haven't evolved to understand 
27.763	the world at that scale.
32.157	Instead, we're trapped on this
32.157	very thin slice of perception
36.377	right in the middle.
38.723	But it gets strange, because even at 
38.723	that slice of reality that we call home,
43.191	we're not seeing most
43.191	of the action that's going on.
46.176	So take the colors of our world.
49.566	This is light waves, electromagnetic
49.566	radiation that bounces off objects
54.279	and it hits specialized receptors
54.279	in the back of our eyes.
57.716	But we're not seeing
57.716	all the waves out there.
61.361	In fact, what we see
63.056	is less than a 10 trillionth
63.056	of what's out there.
67.119	So you have radio waves and microwaves
70.486	and X-rays and gamma rays
70.486	passing through your body right now
73.783	and you're completely unaware of it,
76.732	because you don't come with
76.732	the proper biological receptors
79.913	for picking it up.
81.631	There are thousands
81.631	of cell phone conversations
84.198	passing through you right now,
85.754	and you're utterly blind to it.
88.055	Now, it's not that these things
88.055	are inherently unseeable.
91.953	Snakes include some infrared
91.953	in their reality,
96.852	and honeybees include ultraviolet
96.852	in their view of the world,
100.73	and of course we build machines
100.73	in the dashboards of our cars
103.655	to pick up on signals
103.655	in the radio frequency range,
106.883	and we built machines in hospitals
106.883	to pick up on the X-ray range.
110.575	But you can't sense
110.575	any of those by yourself,
113.965	at least not yet,
115.474	because you don't come equipped
115.474	with the proper sensors.
119.421	Now, what this means is that
119.421	our experience of reality
123.902	is constrained by our biology,
127.362	and that goes against
127.362	the common sense notion
129.916	that our eyes and our ears
129.916	and our fingertips
132.179	are just picking up
132.179	the objective reality that's out there.
136.394	Instead, our brains are sampling
136.394	just a little bit of the world.
142.013	Now, across the animal kingdom,
144.08	different animals pick up
144.08	on different parts of reality.
147.4	So in the blind
147.4	and deaf world of the tick,
150.349	the important signals
150.349	are temperature and butyric acid;
154.83	in the world of the black ghost knifefish,
157.756	its sensory world is lavishly colored
157.756	by electrical fields;
162.655	and for the echolocating bat,
165.116	its reality is constructed
165.116	out of air compression waves.
169.156	That's the slice of their ecosystem
169.156	that they can pick up on,
173.521	and we have a word for this in science.
175.403	It's called the umwelt,
176.911	which is the German word
176.911	for the surrounding world.
180.603	Now, presumably, every animal assumes
183.598	that its umwelt is the entire
183.598	objective reality out there,
187.987	because why would you ever stop to imagine
190.281	that there's something beyond
190.281	what we can sense.
193.412	Instead, what we all do
193.412	is we accept reality
196.126	as it's presented to us.
199.222	Let's do a consciousness-raiser on this.
201.717	Imagine that you are a bloodhound dog.
204.973	Your whole world is about smelling.
207.178	You've got a long snout that has
211.59	and you have wet nostrils
211.59	that attract and trap scent molecules,
216.094	and your nostrils even have slits
216.094	so you can take big nosefuls of air.
220.088	Everything is about smell for you.
223.362	So one day, you stop in your tracks
223.362	with a revelation.
227.263	You look at your human owner
227.263	and you think,
230.583	"""What is it like to have the pitiful,"
230.583	impoverished nose of a human?
235.393	(Laughter)
237.083	What is it like when you take
237.083	a feeble little noseful of air?
240.335	How can you not know that there's
240.335	a cat 100 yards away,
244.384	or that your neighbor was on
244.384	"this very spot six hours ago?"""
247.718	(Laughter)
250.458	So because we're humans,
252.757	we've never experienced
252.757	that world of smell,
255.404	so we don't miss it,
258.083	because we are firmly settled
258.083	into our umwelt.
262.114	But the question is,
262.114	do we have to be stuck there?
266.317	So as a neuroscientist, I'm interested
266.317	in the way that technology
270.845	might expand our umwelt,
273.468	and how that's going to change
273.468	the experience of being human.
278.228	So we already know that we can marry
278.228	our technology to our biology,
281.781	because there are hundreds of thousands
281.781	of people walking around
285.565	with artificial hearing
285.565	and artificial vision.
289.164	So the way this works is, you take
289.164	a microphone and you digitize the signal,
293.553	and you put an electrode strip
293.553	directly into the inner ear.
297.291	Or, with the retinal implant,
297.291	you take a camera
299.59	and you digitize the signal,
299.59	and then you plug an electrode grid
302.864	directly into the optic nerve.
305.882	And as recently as 15 years ago,
309.806	there were a lot of scientists who thought
309.806	these technologies wouldn't work.
313.544	Why? It's because these technologies
313.544	speak the language of Silicon Valley,
318.723	and it's not exactly the same dialect
318.723	as our natural biological sense organs.
324.295	But the fact is that it works;
326.71	the brain figures out
326.71	how to use the signals just fine.
331.719	Now, how do we understand that?
333.763	Well, here's the big secret:
335.458	Your brain is not hearing
335.458	or seeing any of this.
340.728	Your brain is locked in a vault of silence
340.728	and darkness inside your skull.
347.183	All it ever sees are
347.183	electrochemical signals
350.991	that come in along different data cables,
353.54	and this is all it has to work with,
353.54	and nothing more.
358.672	Now, amazingly,
360.924	the brain is really good
360.924	at taking in these signals
363.687	and extracting patterns
363.687	and assigning meaning,
367.238	so that it takes this inner cosmos
367.238	and puts together a story
371.292	of this, your subjective world.
376.179	But here's the key point:
378.129	Your brain doesn't know,
378.129	and it doesn't care,
381.519	where it gets the data from.
384.561	Whatever information comes in,
384.561	it just figures out what to do with it.
389.414	And this is a very efficient
389.414	kind of machine.
391.852	It's essentially a general purpose
391.852	computing device,
396.008	and it just takes in everything
398.423	and figures out
398.423	what it's going to do with it,
401.023	and that, I think, frees up Mother Nature
404.669	to tinker around with different
404.669	sorts of input channels.
409.452	So I call this the P.H. 
409.452	model of evolution,
412.284	and I don't want to get
412.284	too technical here,
414.328	but P.H. stands for Potato Head,
417.369	and I use this name to emphasize
417.369	that all these sensors
421.2	that we know and love, like our eyes
421.2	and our ears and our fingertips,
424.451	these are merely peripheral
424.451	plug-and-play devices:
428.77	You stick them in, and you're good to go.
432.044	The brain figures out what to do
432.044	with the data that comes in.
438.243	And when you look across
438.243	the animal kingdom,
440.449	you find lots of peripheral devices.
443.096	So snakes have heat pits
443.096	with which to detect infrared,
447.206	and the ghost knifefish has
447.206	electroreceptors,
450.456	and the star-nosed mole has this appendage
453.057	with 22 fingers on it
455.704	with which it feels around and constructs
455.704	a 3D model of the world,
459.373	and many birds have magnetite
459.373	so they can orient
463.297	to the magnetic field of the planet.
465.792	So what this means is that
465.792	nature doesn't have to continually
469.664	redesign the brain.
472.079	Instead, with the principles
472.079	of brain operation established,
476.56	all nature has to worry about
476.56	is designing new peripherals.
481.239	Okay. So what this means is this:
484.164	The lesson that surfaces
486.184	is that there's nothing
486.184	really special or fundamental
489.853	about the biology that we
489.853	come to the table with.
492.848	It's just what we have inherited
494.915	from a complex road of evolution.
498.142	But it's not what we have to stick with,
501.671	and our best proof of principle of this
503.715	comes from what's called
503.715	sensory substitution.
506.315	And that refers to feeding
506.315	information into the brain
509.543	via unusual sensory channels,
512.329	and the brain just figures out
512.329	what to do with it.
515.208	Now, that might sound speculative,
517.669	but the first paper demonstrating this was
517.669	published in the journal Nature in 1969.
523.985	So a scientist named Paul Bach-y-Rita
526.353	put blind people
526.353	in a modified dental chair,
529.581	and he set up a video feed,
531.926	and he put something
531.926	in front of the camera,
534.178	and then you would feel that
536.639	poked into your back
536.639	with a grid of solenoids.
539.565	So if you wiggle a coffee cup
539.565	in front of the camera,
542.049	you're feeling that in your back,
544.394	and amazingly, blind people
544.394	got pretty good
547.482	at being able to determine
547.482	what was in front of the camera
551.035	just by feeling it
551.035	in the small of their back.
554.82	Now, there have been many
554.82	modern incarnations of this.
558.326	The sonic glasses take a video feed
558.326	right in front of you
561.6	and turn that into a sonic landscape,
564.455	so as things move around,
564.455	and get closer and farther,
566.956	"it sounds like ""Bzz, bzz, bzz."""
569.03	It sounds like a cacophony,
571.003	but after several weeks, blind people
571.003	start getting pretty good
574.997	at understanding what's in front of them
577.319	just based on what they're hearing.
579.966	And it doesn't have to be
579.966	through the ears:
581.99	this system uses an electrotactile grid
581.99	on the forehead,
585.354	so whatever's in front of the video feed,
585.354	you're feeling it on your forehead.
589.044	Why the forehead? Because you're not
589.044	using it for much else.
591.897	The most modern incarnation
591.897	is called the brainport,
596.103	and this is a little electrogrid
596.103	that sits on your tongue,
599.852	and the video feed gets turned into
599.852	these little electrotactile signals,
603.968	and blind people get so good at using this
603.968	that they can throw a ball into a basket,
610.455	or they can navigate
610.455	complex obstacle courses.
615.311	They can come to see through their tongue.
619.525	Now, that sounds completely insane, right?
621.731	But remember, all vision ever is
624.54	is electrochemical signals
624.54	coursing around in your brain.
628.557	Your brain doesn't know
628.557	where the signals come from.
631.251	It just figures out what to do with them.
634.687	So my interest in my lab
634.687	is sensory substitution for the deaf,
640.493	and this is a project I've undertaken
643.232	with a graduate student
643.232	in my lab, Scott Novich,
646.227	who is spearheading this for his thesis.
648.526	And here is what we wanted to do:
650.522	we wanted to make it so that
650.522	sound from the world gets converted
654.516	in some way so that a deaf person
654.516	can understand what is being said.
659.392	And we wanted to do this, given the power
659.392	and ubiquity of portable computing,
663.92	we wanted to make sure that this
663.92	would run on cell phones and tablets,
668.796	and also we wanted
668.796	to make this a wearable,
671.094	something that you could wear
671.094	under your clothing.
674.136	So here's the concept.
677.326	So as I'm speaking, my sound
677.326	is getting captured by the tablet,
682.402	and then it's getting mapped onto a vest
682.402	that's covered in vibratory motors,
688.16	just like the motors in your cell phone.
691.597	So as I'm speaking,
693.988	the sound is getting translated
693.988	to a pattern of vibration on the vest.
700.327	Now, this is not just conceptual:
701.906	this tablet is transmitting Bluetooth,
701.906	and I'm wearing the vest right now.
707.014	So as I'm speaking -- (Applause) --
710.033	the sound is getting translated
710.033	into dynamic patterns of vibration.
715.966	I'm feeling the sonic world around me.
721.34	So, we've been testing this
721.34	with deaf people now,
725.404	and it turns out that after
725.404	just a little bit of time,
728.91	people can start feeling,
728.91	they can start understanding
732.3	the language of the vest.
734.97	So this is Jonathan. He's 37 years old.
734.97	He has a master's degree.
739.753	He was born profoundly deaf,
742.098	which means that there's a part
742.098	of his umwelt that's unavailable to him.
746.208	So we had Jonathan train with the vest
746.208	for four days, two hours a day,
750.596	and here he is on the fifth day.
753.876	Scott Novich: You.
756.012	David Eagleman: So Scott says a word,
756.012	Jonathan feels it on the vest,
759.226	and he writes it on the board.
762.282	SN: Where. Where.
766.168	DE: Jonathan is able to translate
766.168	this complicated pattern of vibrations
769.805	into an understanding
769.805	of what's being said.
772.684	SN: Touch. Touch.
776.283	DE: Now, he's not doing this --
780.723	(Applause) --
787.944	Jonathan is not doing this consciously,
787.944	because the patterns are too complicated,
792.03	but his brain is starting to unlock
792.03	the pattern that allows it to figure out
797.51	what the data mean,
799.786	and our expectation is that,
799.786	after wearing this for about three months,
803.988	he will have a direct
803.988	perceptual experience of hearing
808.586	in the same way that when a blind person
808.586	passes a finger over braille,
812.765	the meaning comes directly off the page
812.765	without any conscious intervention at all.
818.941	Now, this technology has the potential
818.941	to be a game-changer,
822.494	because the only other solution
822.494	for deafness is a cochlear implant,
826.278	and that requires an invasive surgery.
829.181	And this can be built for 40 times cheaper
829.181	than a cochlear implant,
834.335	which opens up this technology globally,
834.335	even for the poorest countries.
840.052	Now, we've been very encouraged
840.052	by our results with sensory substitution,
845.171	but what we've been thinking a lot about
845.171	is sensory addition.
849.374	How could we use a technology like this
849.374	to add a completely new kind of sense,
854.803	to expand the human umvelt?
857.937	For example, could we feed
857.937	real-time data from the Internet
862.186	directly into somebody's brain,
864.067	and can they develop a direct
864.067	perceptual experience?
867.945	So here's an experiment
867.945	we're doing in the lab.
870.482	A subject is feeling a real-time
870.482	streaming feed from the Net of data
874.376	for five seconds.
876.187	Then, two buttons appear,
876.187	and he has to make a choice.
879.456	He doesn't know what's going on.
881.145	He makes a choice,
881.145	and he gets feedback after one second.
883.841	Now, here's the thing:
885.046	The subject has no idea
885.046	what all the patterns mean,
887.69	but we're seeing if he gets better
887.69	at figuring out which button to press.
891.361	He doesn't know that what we're feeding
893.428	is real-time data from the stock market,
896.609	and he's making buy and sell decisions.
899.116	(Laughter)
901.49	And the feedback is telling him
901.49	whether he did the right thing or not.
904.792	And what we're seeing is,
904.792	can we expand the human umvelt
907.661	so that he comes to have,
907.661	after several weeks,
910.656	a direct perceptual experience
910.656	of the economic movements of the planet.
916.763	So we'll report on that later
916.763	to see how well this goes.
920.129	(Laughter)
922.73	Here's another thing we're doing:
924.82	During the talks this morning,
924.82	we've been automatically scraping Twitter
929.417	for the TED2015 hashtag,
931.855	and we've been doing
931.855	an automated sentiment analysis,
934.548	which means, are people using positive
934.548	words or negative words or neutral?
939.123	And while this has been going on,
941.567	I have been feeling this,
944.562	and so I am plugged in
944.562	to the aggregate emotion
948.835	of thousands of people in real time,
952.991	and that's a new kind of human experience,
952.991	because now I can know
956.729	how everyone's doing
956.729	and how much you're loving this.
960.026	(Laughter) (Applause)
966.899	It's a bigger experience
966.899	than a human can normally have.
971.845	We're also expanding the umvelt of pilots.
974.538	So in this case, the vest is streaming
974.538	nine different measures
978.624	from this quadcopter,
980.25	so pitch and yaw and roll
980.25	and orientation and heading,
983.617	and that improves
983.617	this pilot's ability to fly it.
987.703	It's essentially like he's extending
987.703	his skin up there, far away.
992.998	And that's just the beginning.
994.553	What we're envisioning is taking
994.553	a modern cockpit full of gauges
1000.357	and instead of trying
1000.357	to read the whole thing, you feel it.
1004.908	We live in a world of information now,
1007.393	and there is a difference
1007.393	between accessing big data
1011.201	and experiencing it.
1014.289	So I think there's really no end
1014.289	to the possibilities
1018.114	on the horizon for human expansion.
1020.436	Just imagine an astronaut
1020.436	being able to feel
1025.358	the overall health
1025.358	of the International Space Station,
1028.679	or, for that matter, having you feel
1028.679	the invisible states of your own health,
1033.555	like your blood sugar
1033.555	and the state of your microbiome,
1037.494	or having 360-degree vision
1037.494	or seeing in infrared or ultraviolet.
1043.121	So the key is this:
1043.121	As we move into the future,
1046.616	we're going to increasingly be able
1046.616	to choose our own peripheral devices.
1051.515	We no longer have to wait
1051.515	for Mother Nature's sensory gifts
1055.369	on her timescales,
1057.227	but instead, like any good parent,
1057.227	she's given us the tools that we need
1061.499	to go out and define our own trajectory.
1065.632	So the question now is,
1067.373	how do you want to go out
1067.373	and experience your universe?
1072.598	Thank you.
1074.641	(Applause)
1091.365	Chris Anderson: Can you feel it?
1091.365	DE: Yeah.
1093.553	Actually, this was the first time
1093.553	I felt applause on the vest.
1096.943	It's nice. It's like a massage. (Laughter)
1099.102	CA: Twitter's going crazy.
1099.102	Twitter's going mad.
1102.747	So that stock market experiment.
1105.04	This could be the first experiment
1105.04	that secures its funding forevermore,
1109.568	right, if successful?
1111.563	DE: Well, that's right, I wouldn't
1111.563	have to write to NIH anymore.
1114.715	CA: Well look, just to be
1114.715	skeptical for a minute,
1117.532	I mean, this is amazing,
1117.532	but isn't most of the evidence so far
1120.702	that sensory substitution works,
1123.049	not necessarily 
1123.049	that sensory addition works?
1125.156	I mean, isn't it possible that the
1125.156	blind person can see through their tongue
1128.793	because the visual cortex is still there,
1128.793	ready to process,
1133.971	and that that is needed as part of it?
1135.79	DE: That's a great question.
1135.79	We actually have no idea
1138.434	what the theoretical limits are of what
1138.434	kind of data the brain can take in.
1142.33	The general story, though,
1142.33	is that it's extraordinarily flexible.
1145.402	So when a person goes blind,
1145.402	what we used to call their visual cortex
1149.207	gets taken over by other things,
1149.207	by touch, by hearing, by vocabulary.
1154.265	So what that tells us is that
1154.265	the cortex is kind of a one-trick pony.
1158.327	It just runs certain kinds
1158.327	of computations on things.
1160.975	And when we look around
1160.975	at things like braille, for example,
1164.076	people are getting information
1164.076	through bumps on their fingers.
1167.165	So I don't think we have any reason
1167.165	to think there's a theoretical limit
1170.82	that we know the edge of.
1173.244	CA: If this checks out,
1173.244	you're going to be deluged.
1176.508	There are so many
1176.508	possible applications for this.
1179.759	Are you ready for this? What are you most
1179.759	excited about, the direction it might go?
1183.69	DE: I mean, I think there's
1183.69	a lot of applications here.
1186.267	In terms of beyond sensory substitution,
1186.267	the things I started mentioning
1189.715	about astronauts on the space station,
1189.715	they spend a lot of their time
1194.085	monitoring things, and they could instead
1194.085	just get what's going on,
1197.304	because what this is really good for
1197.304	is multidimensional data.
1200.764	The key is this: Our visual systems
1200.764	are good at detecting blobs and edges,
1205.547	but they're really bad
1205.547	at what our world has become,
1207.995	which is screens
1207.995	with lots and lots of data.
1210.182	We have to crawl that
1210.182	with our attentional systems.
1212.585	So this is a way of just
1212.585	feeling the state of something,
1215.255	just like the way you know the state
1215.255	of your body as you're standing around.
1218.849	So I think heavy machinery, safety,
1218.849	feeling the state of a factory,
1222.028	of your equipment, that's one place
1222.028	it'll go right away.
1225.092	CA: David Eagleman, that was one
1225.092	mind-blowing talk. Thank you very much.
1228.797	DE: Thank you, Chris.
1228.797	(Applause)
