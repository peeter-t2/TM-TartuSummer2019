startsecond	text
12.82	Today I'm going to talk
12.82	about technology and society.
18.86	The Department of Transport
18.86	estimated that last year
	from traffic crashes in the US alone.
27.86	Worldwide, 1.2 million people
27.86	die every year in traffic accidents.
33.58	If there was a way we could eliminate
37.7	would you support it?
39.54	Of course you would.
40.86	This is what driverless car technology
40.86	promises to achieve
44.54	by eliminating the main
44.54	source of accidents --
47.38	human error.
49.74	Now picture yourself
49.74	in a driverless car in the year 2030,
55.18	sitting back and watching
55.18	this vintage TEDxCambridge video.
58.66	(Laughter)
61.34	All of a sudden,
62.58	the car experiences mechanical failure
62.58	and is unable to stop.
67.18	If the car continues,
69.54	it will crash into a bunch
69.54	of pedestrians crossing the street,
74.9	but the car may swerve,
77.059	hitting one bystander,
78.94	killing them to save the pedestrians.
81.86	What should the car do,
81.86	and who should decide?
85.34	What if instead the car
85.34	could swerve into a wall,
88.9	crashing and killing you, the passenger,
92.22	in order to save those pedestrians?
95.06	This scenario is inspired
95.06	by the trolley problem,
98.78	which was invented
98.78	by philosophers a few decades ago
102.58	to think about ethics.
105.94	Now, the way we think
105.94	about this problem matters.
108.46	We may for example
108.46	not think about it at all.
111.1	We may say this scenario is unrealistic,
114.5	incredibly unlikely, or just silly.
117.58	But I think this criticism
117.58	misses the point
120.34	because it takes
120.34	the scenario too literally.
123.74	Of course no accident
123.74	is going to look like this;
126.5	no accident has two or three options
129.86	where everybody dies somehow.
133.3	Instead, the car is going
133.3	to calculate something
135.9	like the probability of hitting
135.9	a certain group of people,
140.82	if you swerve one direction
140.82	versus another direction,
144.18	you might slightly increase the risk
144.18	to passengers or other drivers
147.66	versus pedestrians.
149.22	It's going to be
149.22	a more complex calculation,
152.3	but it's still going
152.3	to involve trade-offs,
155.66	and trade-offs often require ethics.
159.66	We might say then,
159.66	"""Well, let's not worry about this."
162.42	Let's wait until technology
162.42	"is fully ready and 100 percent safe."""
168.34	Suppose that we can indeed
168.34	eliminate 90 percent of those accidents,
172.9	or even 99 percent in the next 10 years.
176.74	What if eliminating
176.74	the last one percent of accidents
179.94	requires 50 more years of research?
184.22	Should we not adopt the technology?
186.54	That's 60 million people
186.54	dead in car accidents
191.34	if we maintain the current rate.
194.58	So the point is,
195.82	waiting for full safety is also a choice,
199.46	and it also involves trade-offs.
203.38	People online on social media
203.38	have been coming up with all sorts of ways
207.74	to not think about this problem.
209.78	One person suggested
209.78	the car should just swerve somehow
213.02	in between the passengers --
215.18	(Laughter)
216.22	and the bystander.
217.5	Of course if that's what the car can do,
217.5	that's what the car should do.
221.74	We're interested in scenarios
221.74	in which this is not possible.
225.1	And my personal favorite
225.1	was a suggestion by a blogger
230.54	to have an eject button in the car
230.54	that you press --
233.58	(Laughter)
234.82	just before the car self-destructs.
236.511	(Laughter)
239.66	So if we acknowledge that cars
239.66	will have to make trade-offs on the road,
246.02	how do we think about those trade-offs,
249.14	and how do we decide?
250.74	Well, maybe we should run a survey
250.74	to find out what society wants,
253.9	because ultimately,
255.38	regulations and the law
255.38	are a reflection of societal values.
259.86	So this is what we did.
261.7	With my collaborators,
263.34	Jean-Fran√ßois Bonnefon and Azim Shariff,
265.7	we ran a survey
267.34	in which we presented people
267.34	with these types of scenarios.
270.219	We gave them two options
270.219	inspired by two philosophers:
274.02	Jeremy Bentham and Immanuel Kant.
277.42	Bentham says the car
277.42	should follow utilitarian ethics:
280.54	it should take the action
280.54	that will minimize total harm --
283.98	even if that action will kill a bystander
286.82	and even if that action
286.82	will kill the passenger.
289.94	Immanuel Kant says the car
289.94	should follow duty-bound principles,
294.94	"like ""Thou shalt not kill."""
297.3	So you should not take an action
297.3	that explicitly harms a human being,
301.78	and you should let the car take its course
304.26	even if that's going to harm more people.
307.46	What do you think?
309.18	Bentham or Kant?
311.58	Here's what we found.
312.86	Most people sided with Bentham.
315.98	So it seems that people
315.98	want cars to be utilitarian,
319.78	minimize total harm,
321.22	and that's what we should all do.
322.82	Problem solved.
325.06	But there is a little catch.
327.74	When we asked people
327.74	whether they would purchase such cars,
331.5	"they said, ""Absolutely not."""
333.14	(Laughter)
335.46	They would like to buy cars
335.46	that protect them at all costs,
339.38	but they want everybody else
339.38	to buy cars that minimize harm.
343.02	(Laughter)
346.54	We've seen this problem before.
348.42	It's called a social dilemma.
350.98	And to understand the social dilemma,
352.82	we have to go a little bit
352.82	back in history.
355.82	In the 1800s,
358.42	English economist William Forster Lloyd
358.42	published a pamphlet
362.18	which describes the following scenario.
364.42	You have a group of farmers --
366.1	English farmers --
367.46	who are sharing a common land
367.46	for their sheep to graze.
371.34	Now, if each farmer
371.34	brings a certain number of sheep --
373.94	let's say three sheep --
375.46	the land will be rejuvenated,
377.58	the farmers are happy,
378.82	the sheep are happy,
380.46	everything is good.
382.26	Now, if one farmer brings one extra sheep,
385.62	that farmer will do slightly better,
385.62	and no one else will be harmed.
390.98	But if every farmer made
390.98	that individually rational decision,
395.66	the land will be overrun,
395.66	and it will be depleted
399.18	to the detriment of all the farmers,
401.38	and of course,
401.38	to the detriment of the sheep.
404.54	We see this problem in many places:
408.9	in the difficulty of managing overfishing,
412.1	or in reducing carbon emissions
412.1	to mitigate climate change.
418.98	When it comes to the regulation
418.98	of driverless cars,
422.9	the common land now
422.9	is basically public safety --
427.26	that's the common good --
429.22	and the farmers are the passengers
431.22	or the car owners who are choosing
431.22	to ride in those cars.
436.78	And by making the individually
436.78	rational choice
439.42	of prioritizing their own safety,
442.26	they may collectively be
442.26	diminishing the common good,
445.42	which is minimizing total harm.
450.14	It's called the tragedy of the commons,
452.3	traditionally,
453.62	but I think in the case
453.62	of driverless cars,
456.74	the problem may be
456.74	a little bit more insidious
459.62	because there is not necessarily
459.62	an individual human being
463.14	making those decisions.
464.86	So car manufacturers
464.86	may simply program cars
468.18	that will maximize safety
468.18	for their clients,
471.9	and those cars may learn
471.9	automatically on their own
474.9	that doing so requires slightly
474.9	increasing risk for pedestrians.
479.34	So to use the sheep metaphor,
480.78	it's like we now have electric sheep
480.78	that have a mind of their own.
484.42	(Laughter)
485.9	And they may go and graze
485.9	even if the farmer doesn't know it.
490.46	So this is what we may call
490.46	the tragedy of the algorithmic commons,
494.46	and if offers new types of challenges.
502.34	Typically, traditionally,
504.26	we solve these types
504.26	of social dilemmas using regulation,
507.62	so either governments
507.62	or communities get together,
510.38	and they decide collectively
510.38	what kind of outcome they want
514.14	and what sort of constraints
514.14	on individual behavior
516.82	they need to implement.
519.42	And then using monitoring and enforcement,
522.06	they can make sure
522.06	that the public good is preserved.
525.26	So why don't we just,
526.859	as regulators,
528.379	require that all cars minimize harm?
531.3	After all, this is
531.3	what people say they want.
535.02	And more importantly,
536.46	I can be sure that as an individual,
539.58	if I buy a car that may
539.58	sacrifice me in a very rare case,
543.46	I'm not the only sucker doing that
545.14	while everybody else
545.14	enjoys unconditional protection.
548.94	In our survey, we did ask people
548.94	whether they would support regulation
552.3	and here's what we found.
554.18	First of all, people
554.18	said no to regulation;
559.1	and second, they said,
560.38	"""Well if you regulate cars to do this"
560.38	and to minimize total harm,
564.34	"I will not buy those cars."""
567.22	So ironically,
568.62	by regulating cars to minimize harm,
572.14	we may actually end up with more harm
574.86	because people may not
574.86	opt into the safer technology
578.54	even if it's much safer
578.54	than human drivers.
582.18	I don't have the final
582.18	answer to this riddle,
585.62	but I think as a starting point,
587.22	we need society to come together
590.54	to decide what trade-offs
590.54	we are comfortable with
594.18	and to come up with ways
594.18	in which we can enforce those trade-offs.
598.34	As a starting point,
598.34	my brilliant students,
600.9	Edmond Awad and Sohan Dsouza,
603.38	built the Moral Machine website,
606.02	which generates random scenarios at you --
609.9	basically a bunch
609.9	of random dilemmas in a sequence
612.38	where you have to choose what
612.38	the car should do in a given scenario.
616.86	And we vary the ages and even
616.86	the species of the different victims.
622.86	So far we've collected
622.86	over five million decisions
626.58	by over one million people worldwide
630.22	from the website.
632.18	And this is helping us
632.18	form an early picture
634.62	of what trade-offs
634.62	people are comfortable with
637.26	and what matters to them --
639.18	even across cultures.
642.06	But more importantly,
643.58	doing this exercise
643.58	is helping people recognize
646.98	the difficulty of making those choices
649.82	and that the regulators
649.82	are tasked with impossible choices.
655.18	And maybe this will help us as a society
655.18	understand the kinds of trade-offs
658.78	that will be implemented
658.78	ultimately in regulation.
661.86	And indeed, I was very happy to hear
663.62	that the first set of regulations
665.66	that came from
665.66	the Department of Transport --
667.82	announced last week --
669.22	included a 15-point checklist
669.22	for all carmakers to provide,
675.82	and number 14 was ethical consideration --
679.1	how are you going to deal with that.
683.62	We also have people
683.62	reflect on their own decisions
686.3	by giving them summaries
686.3	of what they chose.
690.26	I'll give you one example --
691.94	I'm just going to warn you
691.94	that this is not your typical example,
695.5	your typical user.
696.9	This is the most sacrificed and the most
696.9	saved character for this person.
700.54	(Laughter)
706.5	Some of you may agree with him,
708.42	or her, we don't know.
712.3	But this person also seems to slightly
712.3	prefer passengers over pedestrians
718.46	in their choices
720.58	and is very happy to punish jaywalking.
723.42	(Laughter)
729.14	So let's wrap up.
730.379	We started with the question --
730.379	let's call it the ethical dilemma --
733.82	of what the car should do
733.82	in a specific scenario:
736.9	swerve or stay?
739.06	But then we realized
739.06	that the problem was a different one.
741.82	It was the problem of how to get
741.82	society to agree on and enforce
746.38	the trade-offs they're comfortable with.
748.34	It's a social dilemma.
749.62	In the 1940s, Isaac Asimov
749.62	wrote his famous laws of robotics --
754.66	the three laws of robotics.
757.06	A robot may not harm a human being,
759.54	a robot may not disobey a human being,
762.1	and a robot may not allow
762.1	itself to come to harm --
765.38	in this order of importance.
768.18	But after 40 years or so
770.34	and after so many stories
770.34	pushing these laws to the limit,
774.1	Asimov introduced the zeroth law
777.82	which takes precedence above all,
780.1	and it's that a robot
780.1	may not harm humanity as a whole.
784.3	I don't know what this means
784.3	in the context of driverless cars
788.7	or any specific situation,
791.46	and I don't know how we can implement it,
793.7	but I think that by recognizing
795.26	that the regulation of driverless cars
795.26	is not only a technological problem
801.42	but also a societal cooperation problem,
805.62	I hope that we can at least begin
805.62	to ask the right questions.
809.02	Thank you.
810.26	(Applause)
