startsecond	text
12.78	I want you to imagine
14.82	walking into a room,
17.3	a control room with a bunch of people,
19.46	a hundred people, hunched
19.46	over a desk with little dials,
23.1	and that that control room
25.5	will shape the thoughts and feelings
29.22	of a billion people.
32.38	This might sound like science fiction,
35.14	but this actually exists
37.38	right now, today.
39.86	I know because I used to be
39.86	in one of those control rooms.
43.979	I was a design ethicist at Google,
46.3	where I studied how do you ethically
46.3	steer people's thoughts?
50.38	Because what we don't talk about
50.38	is how the handful of people
53.3	working at a handful
53.3	of technology companies
55.86	through their choices will steer
55.86	what a billion people are thinking today.
62.22	Because when you pull out your phone
63.98	and they design how this works
63.98	or what's on the feed,
67.1	it's scheduling little blocks
67.1	of time in our minds.
70.34	If you see a notification,
70.34	it schedules you to have thoughts
73.5	that maybe you didn't intend to have.
76.22	If you swipe over that notification,
78.86	it schedules you into spending
78.86	a little bit of time
81.265	getting sucked into something
82.67	that maybe you didn't intend
82.67	to get sucked into.
87.14	When we talk about technology,
89.86	we tend to talk about it
89.86	as this blue sky opportunity.
92.58	It could go any direction.
95.22	And I want to get serious for a moment
97.1	and tell you why it's going
97.1	in a very specific direction.
100.66	Because it's not evolving randomly.
103.82	There's a hidden goal
103.82	driving the direction
105.86	of all of the technology we make,
108.02	and that goal is the race
108.02	for our attention.
112.66	Because every new site --
115.42	TED, elections, politicians,
118.18	games, even meditation apps --
120.18	have to compete for one thing,
122.98	which is our attention,
124.74	and there's only so much of it.
128.26	And the best way to get people's attention
130.7	is to know how someone's mind works.
133.62	And there's a whole bunch
133.62	of persuasive techniques
135.98	that I learned in college at a lab
135.98	called the Persuasive Technology Lab
139.5	to get people's attention.
141.7	A simple example is YouTube.
143.82	YouTube wants to maximize
143.82	how much time you spend.
146.78	And so what do they do?
148.66	They autoplay the next video.
151.58	And let's say that works really well.
153.42	They're getting a little bit
153.42	more of people's time.
155.86	Well, if you're Netflix,
155.86	you look at that and say,
158.26	well, that's shrinking my market share,
160.142	so I'm going to autoplay the next episode.
163.14	But then if you're Facebook,
164.54	you say, that's shrinking
164.54	all of my market share,
166.9	so now I have to autoplay
166.9	all the videos in the newsfeed
169.58	before waiting for you to click play.
172.14	So the internet is not evolving at random.
176.14	The reason it feels
176.14	like it's sucking us in the way it is
180.58	is because of this race for attention.
182.98	We know where this is going.
184.42	Technology is not neutral,
187.14	and it becomes this race
187.14	to the bottom of the brain stem
190.58	of who can go lower to get it.
193.74	Let me give you an example of Snapchat.
196.1	If you didn't know,
196.1	Snapchat is the number one way
199.82	that teenagers in
199.82	the United States communicate.
202.1	So if you're like me, and you use
202.1	text messages to communicate,
206.3	Snapchat is that for teenagers,
208.1	and there's, like,
208.1	a hundred million of them that use it.
210.82	And they invented
210.82	a feature called Snapstreaks,
213.06	which shows the number of days in a row
214.98	that two people have
214.98	communicated with each other.
217.62	In other words, what they just did
219.5	is they gave two people
219.5	something they don't want to lose.
223.82	Because if you're a teenager,
223.82	and you have 150 days in a row,
227.3	you don't want that to go away.
229.3	And so think of the little blocks of time
229.3	that that schedules in kids' minds.
233.98	This isn't theoretical:
233.98	when kids go on vacation,
236.34	it's been shown they give their passwords
236.34	to up to five other friends
239.62	to keep their Snapstreaks going,
241.86	even when they can't do it.
243.9	And they have, like, 30 of these things,
245.86	and so they have to get through
245.86	taking photos of just pictures or walls
249.26	or ceilings just to get through their day.
253.02	So it's not even like
253.02	they're having real conversations.
255.74	We have a temptation to think about this
257.7	as, oh, they're just using Snapchat
260.42	the way we used to
260.42	gossip on the telephone.
262.46	It's probably OK.
264.3	Well, what this misses
264.3	is that in the 1970s,
266.58	when you were just
266.58	gossiping on the telephone,
269.219	there wasn't a hundred engineers
269.219	on the other side of the screen
272.26	who knew exactly
272.26	how your psychology worked
274.34	and orchestrated you
274.34	into a double bind with each other.
278.26	Now, if this is making you
278.26	feel a little bit of outrage,
282.5	notice that that thought
282.5	just comes over you.
285.1	Outrage is a really good way also
285.1	of getting your attention,
289.7	because we don't choose outrage.
291.3	It happens to us.
292.74	And if you're the Facebook newsfeed,
294.62	whether you'd want to or not,
296.06	you actually benefit when there's outrage.
298.82	Because outrage
298.82	doesn't just schedule a reaction
301.78	in emotional time, space, for you.
305.26	We want to share that outrage
305.26	with other people.
307.7	So we want to hit share and say,
309.3	"""Can you believe the thing"
309.3	"that they said?"""
312.34	And so outrage works really well
312.34	at getting attention,
315.74	such that if Facebook had a choice
315.74	between showing you the outrage feed
319.66	and a calm newsfeed,
321.94	they would want
321.94	to show you the outrage feed,
324.1	not because someone
324.1	consciously chose that,
326.18	but because that worked better
326.18	at getting your attention.
330.94	And the newsfeed control room
330.94	is not accountable to us.
336.86	It's only accountable
336.86	to maximizing attention.
339.18	It's also accountable,
340.42	because of the business model
340.42	of advertising,
342.82	for anybody who can pay the most
342.82	to actually walk into the control room
346.18	"and say, ""That group over there,"
347.78	I want to schedule these thoughts
347.78	"into their minds."""
351.58	So you can target,
353.86	you can precisely target a lie
355.82	directly to the people
355.82	who are most susceptible.
359.9	And because this is profitable,
359.9	it's only going to get worse.
364.86	So I'm here today
367.98	because the costs are so obvious.
372.1	I don't know a more urgent
372.1	problem than this,
374.26	because this problem
374.26	is underneath all other problems.
378.54	It's not just taking away our agency
381.74	to spend our attention
381.74	and live the lives that we want,
385.54	it's changing the way
385.54	that we have our conversations,
389.1	it's changing our democracy,
390.86	and it's changing our ability
390.86	to have the conversations
393.5	and relationships we want with each other.
396.98	And it affects everyone,
398.78	because a billion people
398.78	have one of these in their pocket.
405.18	So how do we fix this?
408.9	We need to make three radical changes
411.86	to technology and to our society.
415.54	The first is we need to acknowledge
415.54	that we are persuadable.
420.66	Once you start understanding
422.06	that your mind can be scheduled
422.06	into having little thoughts
424.86	or little blocks of time
424.86	that you didn't choose,
427.46	wouldn't we want to use that understanding
429.54	and protect against the way
429.54	that that happens?
432.42	I think we need to see ourselves
432.42	fundamentally in a new way.
435.74	It's almost like a new period
435.74	of human history,
437.98	like the Enlightenment,
439.22	but almost a kind of
439.22	self-aware Enlightenment,
441.46	that we can be persuaded,
444.14	and there might be something
444.14	we want to protect.
447.22	The second is we need new models
447.22	and accountability systems
451.82	so that as the world gets better
451.82	and more and more persuasive over time --
455.34	because it's only going
455.34	to get more persuasive --
457.7	that the people in those control rooms
459.58	are accountable and transparent
459.58	to what we want.
462.06	The only form of ethical
462.06	persuasion that exists
464.78	is when the goals of the persuader
466.74	are aligned with the goals
466.74	of the persuadee.
469.46	And that involves questioning big things,
469.46	like the business model of advertising.
474.54	Lastly,
476.14	we need a design renaissance,
478.9	because once you have
478.9	this view of human nature,
481.98	that you can steer the timelines
481.98	of a billion people --
484.98	just imagine, there's people
484.98	who have some desire
487.74	about what they want to do
487.74	and what they want to be thinking
490.62	and what they want to be feeling
490.62	and how they want to be informed,
493.78	and we're all just tugged
493.78	into these other directions.
496.34	And you have a billion people just tugged
496.34	into all these different directions.
500.06	Well, imagine an entire design renaissance
502.14	that tried to orchestrate
502.14	the exact and most empowering
505.26	time-well-spent way
505.26	for those timelines to happen.
508.42	And that would involve two things:
510.1	one would be protecting
510.1	against the timelines
512.26	that we don't want to be experiencing,
514.14	the thoughts that we
514.14	wouldn't want to be happening,
516.58	so that when that ding happens,
516.58	not having the ding that sends us away;
519.94	and the second would be empowering us
519.94	to live out the timeline that we want.
523.58	So let me give you a concrete example.
526.1	Today, let's say your friend
526.1	cancels dinner on you,
528.58	and you are feeling a little bit lonely.
532.379	And so what do you do in that moment?
534.22	You open up Facebook.
536.78	And in that moment,
538.5	the designers in the control room
538.5	want to schedule exactly one thing,
541.9	which is to maximize how much time
541.9	you spend on the screen.
546.46	Now, instead, imagine if those designers
546.46	created a different timeline
550.38	that was the easiest way,
550.38	using all of their data,
553.9	to actually help you get out
553.9	with the people that you care about?
557.02	Just think, alleviating
557.02	all loneliness in society,
562.46	if that was the timeline that Facebook
562.46	wanted to make possible for people.
565.98	Or imagine a different conversation.
567.719	Let's say you wanted to post
567.719	something supercontroversial on Facebook,
571.06	which is a really important
571.06	thing to be able to do,
573.5	to talk about controversial topics.
575.22	And right now, when there's
575.22	that big comment box,
577.58	it's almost asking you,
577.58	what key do you want to type?
580.98	In other words, it's scheduling
580.98	a little timeline of things
583.82	you're going to continue
583.82	to do on the screen.
585.98	And imagine instead that there was
585.98	another button there saying,
588.98	what would be most
588.98	time well spent for you?
591.06	"And you click ""host a dinner."""
592.66	And right there
592.66	underneath the item it said,
594.78	"""Who wants to RSVP for the dinner?"""
596.5	And so you'd still have a conversation
596.5	about something controversial,
599.78	but you'd be having it in the most
599.78	empowering place on your timeline,
603.54	which would be at home that night
603.54	with a bunch of a friends over
606.58	to talk about it.
608.82	So imagine we're running, like,
608.82	a find and replace
612.82	on all of the timelines
612.82	that are currently steering us
615.42	towards more and more
615.42	screen time persuasively
618.9	and replacing all of those timelines
621.46	with what do we want in our lives.
626.78	It doesn't have to be this way.
630.18	Instead of handicapping our attention,
632.46	imagine if we used all of this data
632.46	and all of this power
635.3	and this new view of human nature
636.94	to give us a superhuman ability to focus
639.82	and a superhuman ability to put
639.82	our attention to what we cared about
643.98	and a superhuman ability
643.98	to have the conversations
646.62	that we need to have for democracy.
651.42	The most complex challenges in the world
656.1	require not just us
656.1	to use our attention individually.
660.26	They require us to use our attention
660.26	and coordinate it together.
664.26	Climate change is going to require
664.26	that a lot of people
667.1	are being able
667.1	to coordinate their attention
669.22	in the most empowering way together.
671.14	And imagine creating
671.14	a superhuman ability to do that.
678.82	Sometimes the world's
678.82	most pressing and important problems
683.86	are not these hypothetical future things
683.86	that we could create in the future.
688.38	Sometimes the most pressing problems
690.14	are the ones that are
690.14	right underneath our noses,
692.5	the things that are already directing
692.5	a billion people's thoughts.
696.42	And maybe instead of getting excited
696.42	about the new augmented reality
699.82	and virtual reality
699.82	and these cool things that could happen,
703.14	which are going to be susceptible
703.14	to the same race for attention,
706.46	if we could fix the race for attention
708.66	on the thing that's already
708.66	in a billion people's pockets.
711.86	Maybe instead of getting excited
713.46	about the most exciting
713.46	new cool fancy education apps,
717.66	we could fix the way
717.66	kids' minds are getting manipulated
720.58	into sending empty messages
720.58	back and forth.
723.86	(Applause)
728.18	Maybe instead of worrying
729.46	about hypothetical future
729.46	runaway artificial intelligences
733.26	that are maximizing for one goal,
736.5	we could solve the runaway
736.5	artificial intelligence
739.18	that already exists right now,
741.26	which are these newsfeeds
741.26	maximizing for one thing.
745.9	It's almost like instead of running away
745.9	to colonize new planets,
749.74	we could fix the one
749.74	that we're already on.
751.82	(Applause)
759.86	Solving this problem
761.66	is critical infrastructure
761.66	for solving every other problem.
766.42	There's nothing in your life
766.42	or in our collective problems
770.46	that does not require our ability
770.46	to put our attention where we care about.
775.62	At the end of our lives,
778.06	all we have is our attention and our time.
781.62	What will be time well spent for ours?
783.54	Thank you.
784.78	(Applause)
797.58	Chris Anderson: Tristan, thank you.
797.58	Hey, stay up here a sec.
800.54	First of all, thank you.
801.9	I know we asked you to do this talk
801.9	on pretty short notice,
804.7	and you've had quite a stressful week
806.94	getting this thing together, so thank you.
810.5	Some people listening might say,
810.5	what you complain about is addiction,
814.5	and all these people doing this stuff,
814.5	for them it's actually interesting.
818.02	All these design decisions
819.3	have built user content
819.3	that is fantastically interesting.
822.42	The world's more interesting
822.42	than it ever has been.
824.86	What's wrong with that?
826.14	Tristan Harris:
826.14	I think it's really interesting.
828.42	One way to see this
828.42	is if you're just YouTube, for example,
832.46	you want to always show
832.46	the more interesting next video.
835.14	You want to get better and better
835.14	at suggesting that next video,
838.18	but even if you could propose
838.18	the perfect next video
840.66	that everyone would want to watch,
842.34	it would just be better and better
842.34	at keeping you hooked on the screen.
845.7	So what's missing in that equation
847.38	is figuring out what
847.38	our boundaries would be.
849.54	You would want YouTube to know
849.54	something about, say, falling asleep.
852.78	The CEO of Netflix recently said,
854.42	"""our biggest competitors"
854.42	"are Facebook, YouTube and sleep."""
857.18	And so what we need to recognize
857.18	is that the human architecture is limited
861.66	and that we have certain boundaries
861.66	or dimensions of our lives
864.66	that we want to be honored and respected,
866.66	and technology could help do that.
868.5	(Applause)
871.14	CA: I mean, could you make the case
872.86	that part of the problem here is that
872.86	we've got a na√Øve model of human nature?
878.94	So much of this is justified
878.94	in terms of human preference,
881.7	where we've got these algorithms
881.7	that do an amazing job
884.34	of optimizing for human preference,
886.06	but which preference?
887.42	There's the preferences
887.42	of things that we really care about
890.94	when we think about them
892.34	versus the preferences
892.34	of what we just instinctively click on.
895.42	If we could implant that more nuanced
895.42	view of human nature in every design,
900.1	would that be a step forward?
901.58	TH: Absolutely. I mean, I think right now
903.58	it's as if all of our technology
903.58	is basically only asking our lizard brain
907.1	what's the best way
907.1	to just impulsively get you to do
909.62	the next tiniest thing with your time,
911.78	instead of asking you in your life
913.46	what we would be most
913.46	time well spent for you?
915.66	What would be the perfect timeline
915.66	that might include something later,
918.98	would be time well spent for you
918.98	here at TED in your last day here?
922.18	CA: So if Facebook and Google
922.18	and everyone said to us first up,
925.18	"""Hey, would you like us"
925.18	to optimize for your reflective brain
928.1	"or your lizard brain? You choose."""
929.78	TH: Right. That would be one way. Yes.
934.178	CA: You said persuadability,
934.178	that's an interesting word to me
937.06	because to me there's
937.06	two different types of persuadability.
939.94	There's the persuadability
939.94	that we're trying right now
942.5	of reason and thinking
942.5	and making an argument,
944.7	but I think you're almost
944.7	talking about a different kind,
947.41	a more visceral type of persuadability,
949.34	of being persuaded without
949.34	even knowing that you're thinking.
952.26	TH: Exactly. The reason
952.26	I care about this problem so much is
955.14	I studied at a lab called
955.14	the Persuasive Technology Lab at Stanford
958.34	that taught [students how to recognize]
958.34	exactly these techniques.
960.926	There's conferences and workshops
960.926	that teach people all these covert ways
963.94	of getting people's attention
963.94	and orchestrating people's lives.
966.94	And it's because most people
966.94	don't know that that exists
969.62	that this conversation is so important.
971.54	CA: Tristan, you and I, we both know
971.54	so many people from all these companies.
975.34	There are actually many here in the room,
977.34	and I don't know about you,
977.34	but my experience of them
979.841	is that there is
979.841	no shortage of good intent.
981.94	People want a better world.
984.14	They are actually -- they really want it.
988.14	And I don't think anything you're saying
988.14	is that these are evil people.
992.34	It's a system where there's
992.34	these unintended consequences
996.06	that have really got out of control --
997.94	TH: Of this race for attention.
999.46	It's the classic race to the bottom
999.46	when you have to get attention,
1002.66	and it's so tense.
1003.9	The only way to get more
1003.9	is to go lower on the brain stem,
1006.66	to go lower into outrage,
1006.66	to go lower into emotion,
1009.1	to go lower into the lizard brain.
1010.82	CA: Well, thank you so much for helping us
1010.82	all get a little bit wiser about this.
1014.66	Tristan Harris, thank you.
1014.66	TH: Thank you very much.
1017.1	(Applause)
