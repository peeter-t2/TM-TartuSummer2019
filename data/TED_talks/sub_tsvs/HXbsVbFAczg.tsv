startsecond	text
12.38	Whether you like it or not,
13.74	radical transparency and algorithmic
13.74	decision-making is coming at you fast,
19.14	and it's going to change your life.
21.14	That's because it's now easy
21.14	to take algorithms
23.98	and embed them into computers
25.9	and gather all that data
25.9	that you're leaving on yourself
28.86	all over the place,
30.26	and know what you're like,
31.98	and then direct the computers
31.98	to interact with you
34.94	in ways that are better
34.94	than most people can.
37.98	Well, that might sound scary.
39.62	I've been doing this for a long time
39.62	and I have found it to be wonderful.
43.979	My objective has been
43.979	to have meaningful work
46.66	and meaningful relationships
46.66	with the people I work with,
49.54	and I've learned that I couldn't have that
51.62	unless I had that radical transparency
51.62	and that algorithmic decision-making.
56.5	I want to show you why that is,
58.54	I want to show you how it works.
60.26	And I warn you that some of the things
60.26	that I'm going to show you
63.38	probably are a little bit shocking.
65.58	Since I was a kid,
65.58	I've had a terrible rote memory.
69.94	And I didn't like following instructions,
72.14	I was no good at following instructions.
74.58	But I loved to figure out
74.58	how things worked for myself.
78.5	When I was 12,
79.9	I hated school but I fell in love
79.9	with trading the markets.
83.74	I caddied at the time,
85.42	earned about five dollars a bag.
87.02	And I took my caddying money,
87.02	and I put it in the stock market.
91.06	And that was just because
91.06	the stock market was hot at the time.
94.46	And the first company I bought
95.94	was a company by the name
95.94	of Northeast Airlines.
99.18	Northeast Airlines was
99.18	the only company I heard of
101.94	that was selling for less
101.94	than five dollars a share.
104.66	(Laughter)
106.66	And I figured I could buy more shares,
108.54	and if it went up, I'd make more money.
110.66	So, it was a dumb strategy, right?
114.18	But I tripled my money,
115.66	and I tripled my money
115.66	because I got lucky.
118.34	The company was about to go bankrupt,
120.18	but some other company acquired it,
122.3	and I tripled my money.
123.78	And I was hooked.
125.54	"And I thought, ""This game is easy."""
129.02	With time,
130.26	I learned this game is anything but easy.
132.7	In order to be an effective investor,
134.86	one has to bet against the consensus
137.78	and be right.
139.06	And it's not easy to bet
139.06	against the consensus and be right.
141.94	One has to bet against
141.94	the consensus and be right
144.3	because the consensus
144.3	is built into the price.
147.94	And in order to be an entrepreneur,
150.42	a successful entrepreneur,
152.06	one has to bet against
152.06	the consensus and be right.
157.22	I had to be an entrepreneur
157.22	and an investor --
160.18	and what goes along with that
160.18	is making a lot of painful mistakes.
165.26	So I made a lot of painful mistakes,
168.1	and with time,
169.38	my attitude about those mistakes
169.38	began to change.
172.98	I began to think of them as puzzles.
175.1	That if I could solve the puzzles,
177.06	they would give me gems.
178.98	And the puzzles were:
180.66	What would I do differently in the future
180.66	so I wouldn't make that painful mistake?
185.1	And the gems were principles
187.7	that I would then write down
187.7	so I would remember them
190.86	that would help me in the future.
192.82	And because I wrote them down so clearly,
195.54	I could then --
196.9	eventually discovered --
198.5	I could then embed them into algorithms.
203.22	And those algorithms
203.22	would be embedded in computers,
206.7	and the computers would
206.7	make decisions along with me;
210.06	and so in parallel,
210.06	we would make these decisions.
213.22	And I could see how those decisions
213.22	then compared with my own decisions,
217.22	and I could see that
217.22	those decisions were a lot better.
220.34	And that was because the computer
220.34	could make decisions much faster,
225.1	it could process a lot more information
227.38	and it can process decisions much more --
231.7	less emotionally.
234.58	So it radically improved
234.58	my decision-making.
240.26	Eight years after I started Bridgewater,
245.18	I had my greatest failure,
246.74	my greatest mistake.
249.5	It was late 1970s,
251.66	I was 34 years old,
253.66	and I had calculated that American banks
257.34	had lent much more money
257.34	to emerging countries
260.22	than those countries
260.22	were going to be able to pay back
263.06	and that we would have
263.06	the greatest debt crisis
265.78	since the Great Depression.
268.02	And with it, an economic crisis
270.26	and a big bear market in stocks.
273.5	It was a controversial view at the time.
275.98	People thought it was
275.98	kind of a crazy point of view.
279.3	But in August 1982,
281.54	Mexico defaulted on its debt,
284.34	and a number of other countries followed.
286.62	And we had the greatest debt crisis
286.62	since the Great Depression.
290.9	And because I had anticipated that,
293.7	I was asked to testify to Congress
293.7	"and appear on ""Wall Street Week,"""
298.06	which was the show of the time.
300.06	Just to give you a flavor of that,
300.06	I've got a clip here,
303.02	and you'll see me in there.
306.3	(Video) Mr. Chairman, Mr. Mitchell,
308.02	it's a great pleasure and a great honor
308.02	to be able to appear before you
311.42	in examination with what
311.42	is going wrong with our economy.
315.46	The economy is now flat --
317.42	teetering on the brink of failure.
319.58	Martin Zweig: You were recently
319.58	quoted in an article.
322.1	"You said, ""I can say this"
322.1	with absolute certainty
324.46	"because I know how markets work."""
326.1	Ray Dalio: I can say
326.1	with absolute certainty
328.22	that if you look at the liquidity base
330.1	in the corporations
330.1	and the world as a whole,
333.5	that there's such reduced
333.5	level of liquidity
335.62	that you can't return
335.62	"to an era of stagflation."""
338.86	I look at that now, I think,
338.86	"""What an arrogant jerk!"""
341.98	(Laughter)
345.58	I was so arrogant, and I was so wrong.
348.06	I mean, while the debt crisis happened,
350.66	the stock market and the economy
350.66	went up rather than going down,
354.66	and I lost so much money
354.66	for myself and for my clients
359.7	that I had to shut down
359.7	my operation pretty much,
363.14	I had to let almost everybody go.
365.46	And these were like extended family,
367.22	I was heartbroken.
368.86	And I had lost so much money
370.7	that I had to borrow
370.7	4,000 dollars from my dad
374.06	to help to pay my family bills.
376.66	It was one of the most painful
376.66	experiences of my life ...
381.06	but it turned out to be
381.06	one of the greatest experiences of my life
384.86	because it changed my attitude
384.86	about decision-making.
388.18	"Rather than thinking, ""I'm right,"""
391.26	I started to ask myself,
392.86	"""How do I know I'm right?"""
396.3	I gained a humility that I needed
398.26	in order to balance my audacity.
401.7	I wanted to find the smartest
401.7	people who would disagree with me
405.94	to try to understand their perspective
407.86	or to have them
407.86	stress test my perspective.
411.22	I wanted to make an idea meritocracy.
414.02	In other words,
415.26	not an autocracy in which
415.26	I would lead and others would follow
419.1	and not a democracy in which everybody's
419.1	points of view were equally valued,
422.74	but I wanted to have an idea meritocracy
422.74	in which the best ideas would win out.
427.86	And in order to do that,
429.14	I realized that we would need
429.14	radical truthfulness
432.74	and radical transparency.
434.38	What I mean by radical truthfulness
434.38	and radical transparency
438.26	is people needed to say
438.26	what they really believed
440.94	and to see everything.
443.3	And we literally
443.3	tape almost all conversations
447.26	and let everybody see everything,
448.9	because if we didn't do that,
450.34	we couldn't really have
450.34	an idea meritocracy.
454.58	In order to have an idea meritocracy,
458.3	we have let people speak
458.3	and say what they want.
460.7	Just to give you an example,
462.1	this is an email from Jim Haskel --
464.82	somebody who works for me --
466.22	and this was available
466.22	to everybody in the company.
469.62	"""Ray, you deserve a 'D-'"
472.18	for your performance
472.18	today in the meeting ...
474.46	you did not prepare at all well
476.18	because there is no way
476.18	"you could have been that disorganized."""
481.34	Isn't that great?
482.58	(Laughter)
483.82	That's great.
485.06	It's great because, first of all,
485.06	I needed feedback like that.
488.02	I need feedback like that.
489.66	And it's great because if I don't let Jim,
489.66	and people like Jim,
493.14	to express their points of view,
494.74	our relationship wouldn't be the same.
496.82	And if I didn't make that public
496.82	for everybody to see,
499.9	we wouldn't have an idea meritocracy.
503.58	So for that last 25 years
503.58	that's how we've been operating.
507.46	We've been operating
507.46	with this radical transparency
510.54	and then collecting these principles,
512.86	largely from making mistakes,
514.94	and then embedding
514.94	those principles into algorithms.
519.38	And then those algorithms provide --
522.1	we're following the algorithms
524.14	in parallel with our thinking.
527.1	That has been how we've run
527.1	the investment business,
530.3	and it's how we also deal
530.3	with the people management.
533.06	In order to give you a glimmer
533.06	into what this looks like,
536.82	I'd like to take you into a meeting
539.18	and introduce you to a tool of ours
539.18	"called the ""Dot Collector"""
542.34	that helps us do this.
547.46	A week after the US election,
549.66	our research team held a meeting
551.78	to discuss what a Trump presidency
551.78	would mean for the US economy.
555.82	Naturally, people had
555.82	different opinions on the matter
558.7	and how we were
558.7	approaching the discussion.
561.66	"The ""Dot Collector"" collects these views."
564.46	It has a list of a few dozen attributes,
566.78	so whenever somebody thinks something
566.78	about another person's thinking,
570.82	it's easy for them
570.82	to convey their assessment;
573.78	they simply note the attribute
573.78	and provide a rating from one to 10.
579.34	For example, as the meeting began,
581.62	a researcher named Jen rated me a three --
585.46	in other words, badly --
587.5	(Laughter)
588.9	for not showing a good balance
588.9	of open-mindedness and assertiveness.
593.9	As the meeting transpired,
595.38	Jen's assessments of people
595.38	added up like this.
599.74	Others in the room
599.74	have different opinions.
601.94	That's normal.
603.18	Different people are always
603.18	going to have different opinions.
606.62	And who knows who's right?
609.06	Let's look at just what people thought
609.06	about how I was doing.
613.42	Some people thought I did well,
615.66	others, poorly.
617.9	With each of these views,
619.26	we can explore the thinking
619.26	behind the numbers.
622.34	Here's what Jen and Larry said.
625.58	Note that everyone
625.58	gets to express their thinking,
628.22	including their critical thinking,
629.9	regardless of their position
629.9	in the company.
632.94	Jen, who's 24 years old
632.94	and right out of college,
636.06	can tell me, the CEO,
636.06	that I'm approaching things terribly.
640.3	This tool helps people
640.3	both express their opinions
644.1	and then separate themselves
644.1	from their opinions
647.22	to see things from a higher level.
650.46	When Jen and others shift their attentions
650.46	from inputting their own opinions
655.38	to looking down on the whole screen,
657.98	their perspective changes.
660.5	They see their own opinions
660.5	as just one of many
663.66	and naturally start asking themselves,
666.22	"""How do I know my opinion is right?"""
669.3	That shift in perspective is like going
669.3	from seeing in one dimension
673.38	to seeing in multiple dimensions.
675.66	And it shifts the conversation
675.66	from arguing over our opinions
679.78	to figuring out objective criteria
679.78	for determining which opinions are best.
684.74	"Behind the ""Dot Collector"""
684.74	is a computer that is watching.
688.94	It watches what all
688.94	these people are thinking
691.14	and it correlates that
691.14	with how they think.
693.74	And it communicates advice
693.74	back to each of them based on that.
698.34	Then it draws the data
698.34	from all the meetings
701.78	to create a pointilist painting
701.78	of what people are like
705.02	and how they think.
706.98	And it does that guided by algorithms.
710.62	Knowing what people are like helps
710.62	to match them better with their jobs.
714.94	For example,
716.18	a creative thinker who is unreliable
717.94	might be matched up with someone
717.94	who's reliable but not creative.
722.1	Knowing what people are like
722.1	also allows us to decide
725.46	what responsibilities to give them
727.74	and to weigh our decisions
727.74	based on people's merits.
731.86	We call it their believability.
734.38	Here's an example of a vote that we took
736.38	where the majority
736.38	of people felt one way ...
740.74	but when we weighed the views
740.74	based on people's merits,
743.7	the answer was completely different.
746.74	This process allows us to make decisions
746.74	not based on democracy,
751.34	not based on autocracy,
753.5	but based on algorithms that take
753.5	people's believability into consideration.
761.34	Yup, we really do this.
763.06	(Laughter)
766.38	We do it because it eliminates
769.26	what I believe to be
769.26	one of the greatest tragedies of mankind,
773.74	and that is people arrogantly,
776.58	naïvely holding opinions
776.58	in their minds that are wrong,
781.06	and acting on them,
782.34	and not putting them out there
782.34	to stress test them.
785.82	And that's a tragedy.
787.18	And we do it because it elevates ourselves
787.18	above our own opinions
792.62	so that we start to see things
792.62	through everybody's eyes,
795.54	and we see things collectively.
798.18	Collective decision-making is so much
798.18	better than individual decision-making
802.54	if it's done well.
804.18	It's been the secret sauce
804.18	behind our success.
806.82	It's why we've made
806.82	more money for our clients
809.02	than any other hedge fund in existence
810.98	and made money
815.7	So what's the problem
815.7	with being radically truthful
820.26	and radically transparent with each other?
825.22	People say it's emotionally difficult.
828.06	Critics say it's a formula
828.06	for a brutal work environment.
833.22	Neuroscientists tell me it has to do
833.22	with how are brains are prewired.
838.1	There's a part of our brain
838.1	that would like to know our mistakes
841.34	and like to look at our weaknesses
841.34	so we could do better.
845.94	I'm told that that's
845.94	the prefrontal cortex.
848.86	And then there's a part of our brain
848.86	which views all of this as attacks.
853.74	I'm told that that's the amygdala.
856.26	In other words,
856.26	there are two you's inside you:
859.34	there's an emotional you
860.78	and there's an intellectual you,
862.58	and often they're at odds,
864.38	and often they work against you.
866.98	It's been our experience
866.98	that we can win this battle.
870.74	We win it as a group.
872.82	It takes about 18 months typically
875.18	to find that most people
875.18	prefer operating this way,
878.26	with this radical transparency
880.3	than to be operating
880.3	in a more opaque environment.
883.66	There's not politics,
883.66	there's not the brutality of --
887.98	you know, all of that hidden,
887.98	behind-the-scenes --
890.38	there's an idea meritocracy
890.38	where people can speak up.
893.34	And that's been great.
894.62	It's given us more effective work,
896.3	and it's given us
896.3	more effective relationships.
899.22	But it's not for everybody.
901.5	We found something like
904.46	it's just not for.
906.22	And by the way,
907.46	when I say radical transparency,
909.3	I'm not saying transparency
909.3	about everything.
911.66	I mean, you don't have to tell somebody
911.66	that their bald spot is growing
915.5	or their baby's ugly.
917.14	So, I'm just talking about --
919.26	(Laughter)
920.5	talking about the important things.
922.7	So --
923.94	(Laughter)
928.42	So when you leave this room,
929.86	I'd like you to observe yourself
929.86	in conversations with others.
935.18	Imagine if you knew
935.18	what they were really thinking,
939.58	and imagine if you knew
939.58	what they were really like ...
943.66	and imagine if they knew
943.66	what you were really thinking
947.66	and what were really like.
949.98	It would certainly clear things up a lot
952.58	and make your operations
952.58	together more effective.
955.46	I think it will improve
955.46	your relationships.
958.42	Now imagine that you can have algorithms
961.74	that will help you gather
961.74	all of that information
965.58	and even help you make decisions
965.58	in an idea-meritocratic way.
972.46	This sort of radical transparency
972.46	is coming at you
976.82	and it is going to affect your life.
979.42	And in my opinion,
981.5	it's going to be wonderful.
982.86	So I hope it is as wonderful for you
985.22	as it is for me.
986.98	Thank you very much.
988.26	(Applause)
